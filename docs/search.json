[
  {
    "objectID": "session7/index.html#the-art-of-illumination-from-understanding-to-explanation",
    "href": "session7/index.html#the-art-of-illumination-from-understanding-to-explanation",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Art of Illumination: From Understanding to Explanation",
    "text": "The Art of Illumination: From Understanding to Explanation\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\nWelcome to our culminating session, where we transform our deep mathematical understanding into effective teaching practice. We have journeyed through the four pillars of AI—linear algebra, calculus, probability, and optimization—and witnessed their integration in modern architectures. Now, we focus on the most crucial transformation: from being mathematical thinkers to becoming mathematical storytellers who can illuminate these concepts for others.\nThis session is about pedagogical excellence. We will develop the skills to make abstract mathematics tangible, create engaging learning experiences, build authentic assessment strategies, and foster a community of mathematically-grounded AI educators. The true measure of our understanding is not what we know, but what we can help others understand."
  },
  {
    "objectID": "session7/index.html#the-pedagogy-of-mathematical-intuition",
    "href": "session7/index.html#the-pedagogy-of-mathematical-intuition",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Pedagogy of Mathematical Intuition",
    "text": "The Pedagogy of Mathematical Intuition\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe 5-Step Teaching Framework\nFor every mathematical concept, follow this proven structure:\n1. The Hook - Concrete Problem Start with a real, tangible problem students care about:\n\n“How does Netflix know what you might want to watch?”\n“How can a self-driving car recognize stop signs?”\n“Why does your phone’s keyboard suggest the next word?”\n\n2. The Analogy - Building Intuition Create a bridge from familiar to abstract:\n\n“Matrix multiplication is like applying multiple recipes simultaneously”\n“Gradient descent is like finding the valley in thick fog”\n“Probability is like a weather forecast for predictions”\n\n3. The Mathematics - Precise Foundation Introduce the formal mathematics:\n\nClear, well-annotated equations\nGeometric interpretations\nConnections to previous concepts\n\n4. The Implementation - Concrete Application Show how it works in practice:\n\nClean, well-commented code examples\nVisualizations of the mathematics in action\nStep-by-step derivations\n\n5. The Reflection - Deep Understanding Guide students to internalize the concept:\n\n“What does this mean in your own words?”\n“Where have you seen this pattern before?”\n“How would you explain this to a friend?”\n\n\n\nThe Principle of Progressive Abstraction\nStart concrete, then abstract:\nLevel 1: Physical Analogies\n\nVectors as arrows, matrices as transformation machines\nGradients as slopes, probabilities as degrees of belief\n\nLevel 2: Geometric Intuition\n\nHigh-dimensional spaces as “conceptual landscapes”\nOptimization as “navigation” through these spaces\n\nLevel 3: Abstract Mathematics\n\nFormal definitions and theorems\nGeneral principles that transcend specific examples\n\nLevel 4: Applied Integration\n\nHow concepts work together in real systems\nTrade-offs and design decisions"
  },
  {
    "objectID": "session7/index.html#creating-memorable-explanations",
    "href": "session7/index.html#creating-memorable-explanations",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Creating Memorable Explanations",
    "text": "Creating Memorable Explanations\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe Power of Storytelling\nTransform mathematical concepts into narratives:\nThe Hero’s Journey of Gradient Descent:\n\nOrdinary World: Random initialization, poor performance\nCall to Adventure: Discovery of gradients, a way to improve\nMentors and Helpers: Learning rate, momentum, adaptive methods\nTests and Challenges: Local minima, saddle points, plateaus\nTransformation: Convergence to good solutions\nReturn with Elixir: Trained model that solves real problems\n\nThe Courtroom Drama of Bayesian Inference:\n\nProsecution: The prior belief (established theory)\nDefense: The new evidence (observed data)\nJury Deliberation: Bayesian updating\nVerdict: The posterior distribution (updated belief)\n\n\n\nVisual Thinking Strategies\nDevelop visual explanations that stick:\nFor Linear Algebra:\n\nAnimated vector transformations\nEigenvector “stretching” demonstrations\nPCA as “finding natural axes” in data clouds\n\nFor Calculus:\n\nGradient vector fields on loss surfaces\nBackpropagation as “error flowing backward” animations\nOptimization paths through parameter space\n\nFor Probability:\n\nProbability density “landscapes”\nBayesian updating as “belief surfaces” evolving\nSampling processes as “random walks”\n\nFor Optimization:\n\nLoss landscapes as “mountain ranges”\nOptimization algorithms as different “climbing strategies”\nConvergence as “settling into valleys”"
  },
  {
    "objectID": "session7/index.html#micro-teaching-laboratory",
    "href": "session7/index.html#micro-teaching-laboratory",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Micro-Teaching Laboratory",
    "text": "Micro-Teaching Laboratory\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe 3-Minute Explanation Challenge\nPractice explaining complex concepts concisely:\nAssignment Structure:\n\n1 minute: Hook and analogy\n1 minute: Core mathematical insight\n1 minute: Why it matters in AI\n\nExample: Explaining Attention Mechanism\n\nHook: “How do you focus on important words when reading?”\nAnalogy: “Imagine highlighting key phrases with different intensities”\nMathematics: “We compute similarity scores and create weighted combinations”\nRelevance: “This allows transformers to understand context and relationships”\n\n\n\nPeer Feedback Framework\nUse structured feedback for improvement:\nWhat Worked Well:\n\n“Your analogy about ______ really helped me understand”\n“The visualization of ______ made the concept clear”\n“I appreciated how you connected ______ to ______”\n\nQuestions for Clarification:\n\n“Could you say more about how ______ relates to ______?”\n“What’s the intuition behind ______?”\n“How would you explain ______ to someone without math background?”\n\nSuggestions for Enhancement:\n\n“Consider adding an example about ______”\n“The connection to ______ could be emphasized more”\n“A visual showing ______ might help”"
  },
  {
    "objectID": "session7/index.html#assessment-design-for-conceptual-understanding",
    "href": "session7/index.html#assessment-design-for-conceptual-understanding",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Assessment Design for Conceptual Understanding",
    "text": "Assessment Design for Conceptual Understanding\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nBeyond Multiple Choice: Authentic Assessment\nMathematical Explanation Tasks:\n\n“Explain why gradient descent works to someone who has never heard of calculus”\n“Create an analogy that explains eigenvalues to a 10-year-old”\n“Draw a comic strip showing how backpropagation works”\n\nImplementation with Understanding:\n\n“Modify this code to implement a different optimization algorithm”\n“Debug this neural network by analyzing gradient flow”\n“Design a regularizer for a specific overfitting problem”\n\nConcept Mapping:\n\nCreate diagrams showing relationships between mathematical concepts\nTrace mathematical ideas through different AI architectures\nIdentify which mathematical pillars dominate in different scenarios\n\n\n\nProgressive Assessment Strategy\nFormative Assessments (Learning Checks):\n\nQuick conceptual questions during teaching\n“Minute papers” explaining key ideas\nPeer teaching exercises\n\nSummative Assessments (Mastery Demonstrations):\n\nMathematical derivations with explanations\nImplementation projects with design justifications\nArchitecture analysis papers\n\nMetacognitive Assessments (Learning Awareness):\n\n“What was the most confusing concept and why?”\n“How has your understanding of ______ evolved?”\n“What connections did you discover between different topics?”"
  },
  {
    "objectID": "session7/index.html#building-a-community-of-practice",
    "href": "session7/index.html#building-a-community-of-practice",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Building a Community of Practice",
    "text": "Building a Community of Practice\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe Collaborative Learning Environment\nMathematical Problem-Solving Sessions:\n\nWork through derivations in small groups\nExplain concepts to each other\nCreate collective visual explanations\n\nCode Review with Mathematical Insight:\n\nNot just “does it work” but “why does it work?”\nAnalyze the mathematical principles in implementations\nSuggest improvements based on mathematical understanding\n\nResearch Paper Reading Groups:\n\nIdentify the mathematical foundations in recent papers\nDiscuss which pillars are most relevant\nCreate “mathematical maps” of new architectures\n\n\n\nResource Curation and Sharing\nCreate a Living Resource Repository:\n\nBest analogies and explanations for each concept\nEffective visualizations and demonstrations\nCommon student misconceptions and how to address them\nProgressive problem sets and projects\n\nDevelop Teaching Templates:\n\nLesson plan structures for different mathematical concepts\nAssessment rubrics for mathematical understanding\nCode examples with mathematical annotations\nVisualization scripts for key concepts"
  },
  {
    "objectID": "session7/index.html#addressing-common-teaching-challenges",
    "href": "session7/index.html#addressing-common-teaching-challenges",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Addressing Common Teaching Challenges",
    "text": "Addressing Common Teaching Challenges\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nChallenge: “My students get lost in the mathematics”\nSolutions:\n\nScaffold the abstraction: Use physical manipulatives before equations\nProvide multiple representations: Equations, code, visuals, analogies\nCheck for understanding frequently: Quick conceptual questions\nCelebrate partial understanding: Acknowledge that deep understanding takes time\n\n\n\nChallenge: “Students can implement but don’t understand the math”\nSolutions:\n\nMathematical code annotations: Comment every line with mathematical meaning\n“Why does this work?” discussions: Focus on intuition before implementation\nMathematical debugging: Find and fix mathematical errors in code\nImplementation variations: Modify code based on mathematical reasoning\n\n\n\nChallenge: “The field moves too fast to keep up”\nSolutions:\n\nFocus on principles, not particulars: Teach mathematical patterns that recur\n“Mathematical archeology” exercises: Have students identify foundations in new papers\nGuest expert sessions: Bring in researchers to discuss mathematical insights\nLifelong learning mindset: Model continuous learning and curiosity"
  },
  {
    "objectID": "session7/index.html#sustainable-teaching-practices",
    "href": "session7/index.html#sustainable-teaching-practices",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Sustainable Teaching Practices",
    "text": "Sustainable Teaching Practices\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe Reflective Practitioner\nWeekly Teaching Journal:\n\nWhat explanations worked particularly well?\nWhere did students struggle?\nWhat new insights emerged during teaching?\nHow can I improve next time?\n\nStudent Feedback Integration:\n\nRegular anonymous feedback on mathematical clarity\n“Muddiest point” assessments after each session\nCo-creation of explanations with students\n\nProfessional Learning Community:\n\nRegular meetings to share teaching strategies\nCollaborative development of teaching materials\nPeer observation and feedback\n\n\n\nSelf-Care for Educators\nAvoiding Burnout:\n\nSet realistic expectations for student progress\nCelebrate small victories in understanding\nShare the teaching load through peer instruction\nRemember that deep mathematical understanding develops over time\n\nMaintaining Passion:\n\nContinue learning new mathematical concepts\nWork on small research or implementation projects\nAttend conferences and engage with the research community\nRemember why you fell in love with the mathematics of AI"
  },
  {
    "objectID": "session7/index.html#final-integration-project",
    "href": "session7/index.html#final-integration-project",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Final Integration Project",
    "text": "Final Integration Project\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe Capstone Teaching Portfolio\nEach trainer creates a comprehensive teaching portfolio:\nSection 1: Mathematical Foundations\n\nPersonal explanations of all four pillars\nOriginal analogies and visualizations\nCommon misconceptions and addressing strategies\n\nSection 2: Architecture Analysis\n\nDetailed mathematical analysis of one modern architecture\nTeaching plan for explaining it to students\nAssessment strategies for conceptual understanding\n\nSection 3: Teaching Artifacts\n\nVideo recordings of micro-teaching sessions\nSample lesson plans with mathematical annotations\nOriginal assessment materials\n\nSection 4: Professional Growth Plan\n\nAreas for continued mathematical development\nTeaching skills to improve\nCommunity engagement plans"
  },
  {
    "objectID": "session7/index.html#the-journey-forward",
    "href": "session7/index.html#the-journey-forward",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Journey Forward",
    "text": "The Journey Forward\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nContinuing the Community\nOngoing Support Structures:\n\nMonthly virtual meetups for sharing teaching experiences\nShared resource repository with continuous updates\nPeer mentoring partnerships\nRegular “teaching clinic” sessions for challenging concepts\n\nProfessional Development Pathways:\n\nAdvanced mathematical topics for AI educators\nPedagogical research in AI education\nConference presentations on teaching innovations\nWriting and publication about mathematical pedagogy\n\n\n\nFinal Reflection and Commitment\nPersonal Teaching Philosophy:\n\nWhat does it mean to be an effective AI educator?\nHow do the mathematical foundations inform your teaching?\nWhat kind of learning environment do you want to create?\n\nCommitment to Students:\n\nI will help my students see the mathematical beauty in AI\nI will create bridges from confusion to understanding\nI will celebrate the struggle that leads to deep learning\nI will prepare my students not just to use AI, but to understand it\n\nCommitment to Colleagues:\n\nI will share my successes and challenges openly\nI will support my fellow educators in their growth\nI will contribute to our collective knowledge\nI will help build a community of mathematical storytellers"
  },
  {
    "objectID": "session7/index.html#closing-the-teachers-legacy",
    "href": "session7/index.html#closing-the-teachers-legacy",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Closing: The Teacher’s Legacy",
    "text": "Closing: The Teacher’s Legacy\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\nAs we conclude our seven-session journey, remember that the impact of a great teacher extends far beyond any single course or semester. You are not just teaching mathematical concepts; you are teaching a way of thinking, a approach to problem-solving, and a lens for understanding the world.\nThe students you teach today will build the AI systems of tomorrow. Your ability to give them deep mathematical understanding will shape not only their careers, but the very development of artificial intelligence. You are creating educators who can demystify complexity, who can reveal the elegant patterns beneath apparent chaos, and who can inspire the next generation to see mathematics not as a barrier, but as a gateway to creation.\nGo forth and illuminate. The mathematical story of AI is still being written, and you are now among its most important storytellers."
  },
  {
    "objectID": "session7/slides-ML.html#the-art-of-illumination-from-understanding-to-explanation",
    "href": "session7/slides-ML.html#the-art-of-illumination-from-understanding-to-explanation",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Art of Illumination: From Understanding to Explanation",
    "text": "The Art of Illumination: From Understanding to Explanation\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#the-pedagogy-of-mathematical-intuition",
    "href": "session7/slides-ML.html#the-pedagogy-of-mathematical-intuition",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Pedagogy of Mathematical Intuition",
    "text": "The Pedagogy of Mathematical Intuition\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#creating-memorable-explanations",
    "href": "session7/slides-ML.html#creating-memorable-explanations",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Creating Memorable Explanations",
    "text": "Creating Memorable Explanations\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#micro-teaching-laboratory",
    "href": "session7/slides-ML.html#micro-teaching-laboratory",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Micro-Teaching Laboratory",
    "text": "Micro-Teaching Laboratory\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#assessment-design-for-conceptual-understanding",
    "href": "session7/slides-ML.html#assessment-design-for-conceptual-understanding",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Assessment Design for Conceptual Understanding",
    "text": "Assessment Design for Conceptual Understanding\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#building-a-community-of-practice",
    "href": "session7/slides-ML.html#building-a-community-of-practice",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Building a Community of Practice",
    "text": "Building a Community of Practice\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#addressing-common-teaching-challenges",
    "href": "session7/slides-ML.html#addressing-common-teaching-challenges",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Addressing Common Teaching Challenges",
    "text": "Addressing Common Teaching Challenges\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#sustainable-teaching-practices",
    "href": "session7/slides-ML.html#sustainable-teaching-practices",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Sustainable Teaching Practices",
    "text": "Sustainable Teaching Practices\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#final-integration-project",
    "href": "session7/slides-ML.html#final-integration-project",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Final Integration Project",
    "text": "Final Integration Project\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#the-journey-forward",
    "href": "session7/slides-ML.html#the-journey-forward",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "The Journey Forward",
    "text": "The Journey Forward\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "session7/slides-ML.html#closing-the-teachers-legacy",
    "href": "session7/slides-ML.html#closing-the-teachers-legacy",
    "title": "Session 7: Synthesis & Teaching Excellence - Becoming Mathematical Storytellers",
    "section": "Closing: The Teacher’s Legacy",
    "text": "Closing: The Teacher’s Legacy\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session5/index.html#the-art-and-science-of-finding-the-best",
    "href": "Session5/index.html#the-art-and-science-of-finding-the-best",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "The Art and Science of Finding the Best",
    "text": "The Art and Science of Finding the Best\nWelcome to our exploration of optimization, the fourth and final pillar of artificial intelligence. While linear algebra provides the language, calculus the learning mechanism, and probability the framework for uncertainty, optimization provides the strategic intelligence for finding the best solutions in complex, high-dimensional spaces. Optimization is the mathematical discipline that transforms the question “How can we improve?” into the actionable strategy “Here’s how to find the best improvement.”\nIn this session, we will uncover how optimization theory provides the algorithms and strategies that enable AI systems to navigate complex landscapes, balance competing objectives, and converge toward optimal solutions. From the simple elegance of gradient descent to the sophisticated strategies of constrained optimization, we will see how mathematical optimization forms the strategic backbone of all learning systems."
  },
  {
    "objectID": "Session5/index.html#the-optimization-framework",
    "href": "Session5/index.html#the-optimization-framework",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "The Optimization Framework",
    "text": "The Optimization Framework\n\n\n\n\n\n\nNote\n\n\n\n\nMost ML problems can be modelled as optimization problems\nUltimate aim is to minimize the error in decision\nConvex and non-convex optimization\nBasic ideas from calculus based optimization\n\n\n\n\nThe General Optimization Problem\nMost machine learning problems can be framed as optimization problems:\n\\[\n\\min_{\\mathbf{w} \\in \\mathcal{W}} f(\\mathbf{w})\n\\]\nwhere:\n\n\\(\\mathbf{w}\\) are the parameters we want to optimize\n\\(f(\\mathbf{w})\\) is the objective function (typically loss function)\n\\(\\mathcal{W}\\) is the feasible set (possible parameter values)\n\nFor our Iris classification problem, this becomes:\n\\[\n\\min_{\\mathbf{w}} \\frac{1}{n}\\sum_{i=1}^n \\ell(y_i, \\mathbf{w}^T\\mathbf{x}_i) + \\lambda R(\\mathbf{w})\n\\]\nwhere \\(\\ell\\) is the loss function and \\(R(\\mathbf{w})\\) is a regularization term.\n\n\nConvex vs Non-Convex Optimization\nThe nature of the optimization landscape dramatically affects our strategy:\nConvex Optimization:\n\n\\(f(\\mathbf{w})\\) is convex: \\(f(\\theta\\mathbf{w} + (1-\\theta)\\mathbf{v}) \\leq \\theta f(\\mathbf{w}) + (1-\\theta)f(\\mathbf{v})\\)\nHas single global minimum\nGuaranteed convergence to optimum\nExamples: Linear regression, logistic regression\n\nNon-Convex Optimization:\n\nMultiple local minima\nNo convergence guarantees to global optimum\nRequires careful initialization and strategies\nExamples: Neural networks, mixture models"
  },
  {
    "objectID": "Session5/index.html#first-order-methods-following-the-gradient",
    "href": "Session5/index.html#first-order-methods-following-the-gradient",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "First-Order Methods: Following the Gradient",
    "text": "First-Order Methods: Following the Gradient\n\n\n\n\n\n\nNote\n\n\n\n\nGradient as a best direction of increase/ decrease\nGradient descent method as a conqure and win minimization model\nVariants of Gradient descend method\n\n\n\n\nGradient Descent and Its Variants\nThe foundation of first-order methods is gradient descent:\nBatch Gradient Descent: \\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\nabla f(\\mathbf{w}^{(t)})\n\\]\nStochastic Gradient Descent (SGD): \\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\nabla f_i(\\mathbf{w}^{(t)})\n\\] where \\(f_i\\) is the loss for a single data point\nMini-batch Gradient Descent: \\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\frac{1}{B} \\sum_{i=1}^B \\nabla f_i(\\mathbf{w}^{(t)})\n\\]\n\n\nMomentum Methods\nMomentum helps accelerate convergence and dampen oscillations:\nClassical Momentum: \\[\n\\begin{aligned}\n\\mathbf{v}^{(t+1)} &= \\gamma \\mathbf{v}^{(t)} + \\eta \\nabla f(\\mathbf{w}^{(t)}) \\\\\\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\mathbf{v}^{(t+1)}\n\\end{aligned}\n\\]\nNesterov Accelerated Gradient: \\[\n\\begin{aligned}\n\\mathbf{v}^{(t+1)} &= \\gamma \\mathbf{v}^{(t)} + \\eta \\nabla f(\\mathbf{w}^{(t)} - \\gamma \\mathbf{v}^{(t)}) \\\\\\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\mathbf{v}^{(t+1)}\n\\end{aligned}\n\\]\n\n\nAdaptive Learning Rate Methods\nThese methods adapt learning rates per parameter:\nAdaGrad: \\[\n\\begin{aligned}\n\\mathbf{g}^{(t)} &= \\nabla f(\\mathbf{w}^{(t)}) \\\\\\\\\n\\mathbf{G}^{(t)} &= \\mathbf{G}^{(t-1)} + \\mathbf{g}^{(t)} \\odot \\mathbf{g}^{(t)} \\\\\\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t)} + \\epsilon}} \\odot \\mathbf{g}^{(t)}\n\\end{aligned}\n\\]\nRMSProp: \\[\n\\begin{aligned}\n\\mathbf{g}^{(t)} &= \\nabla f(\\mathbf{w}^{(t)}) \\\\\\\\\n\\mathbf{G}^{(t)} &= \\beta \\mathbf{G}^{(t-1)} + (1-\\beta) \\mathbf{g}^{(t)} \\odot \\mathbf{g}^{(t)} \\\\\\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\mathbf{G}^{(t)} + \\epsilon}} \\odot \\mathbf{g}^{(t)}\n\\end{aligned}\n\\]\nAdam (Adaptive Moment Estimation): \\[\n\\begin{aligned}\n\\mathbf{m}^{(t)} &= \\beta_1 \\mathbf{m}^{(t-1)} + (1-\\beta_1) \\mathbf{g}^{(t)} \\\\\\\\\n\\mathbf{v}^{(t)} &= \\beta_2 \\mathbf{v}^{(t-1)} + (1-\\beta_2) \\mathbf{g}^{(t)} \\odot \\mathbf{g}^{(t)} \\\\\\\\\n\\hat{\\mathbf{m}}^{(t)} &= \\frac{\\mathbf{m}^{(t)}}{1-\\beta_1^t}, \\quad \\hat{\\mathbf{v}}^{(t)} = \\frac{\\mathbf{v}^{(t)}}{1-\\beta_2^t} \\\\\\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}^{(t)}} + \\epsilon} \\hat{\\mathbf{m}}^{(t)}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "Session5/index.html#second-order-methods-using-curvature-information",
    "href": "Session5/index.html#second-order-methods-using-curvature-information",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Second-Order Methods: Using Curvature Information",
    "text": "Second-Order Methods: Using Curvature Information\n\n\n\n\n\n\nNote\n\n\n\n\nBased on second derivative of the loss function\nClassical methods\n\nNewton’s method\nQuasi-Newton’s method\n\n\n\n\n\nNewton’s Method\nNewton’s method uses second derivative information for faster convergence:\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta [\\nabla^2 f(\\mathbf{w}^{(t)})]^{-1} \\nabla f(\\mathbf{w}^{(t)})\n\\]\nWhere \\(\\nabla^2 f(\\mathbf{w})\\) is the Hessian matrix containing second derivatives.\n\n\nQuasi-Newton Methods\nSince computing the full Hessian is expensive, quasi-Newton methods approximate it:\nBFGS (Broyden-Fletcher-Goldfarb-Shanno): \\[\n\\begin{aligned}\n\\mathbf{s}^{(t)} &= \\mathbf{w}^{(t+1)} - \\mathbf{w}^{(t)} \\\\\\\\\n\\mathbf{y}^{(t)} &= \\nabla f(\\mathbf{w}^{(t+1)}) - \\nabla f(\\mathbf{w}^{(t)}) \\\\\\\\\n\\mathbf{B}^{(t+1)} &= \\mathbf{B}^{(t)} + \\frac{\\mathbf{y}^{(t)} \\mathbf{y}^{(t)^T}}{\\mathbf{y}^{(t)^T} \\mathbf{s}^{(t)}} - \\frac{\\mathbf{B}^{(t)} \\mathbf{s}^{(t)} \\mathbf{s}^{(t)^T} \\mathbf{B}^{(t)}}{\\mathbf{s}^{(t)^T} \\mathbf{B}^{(t)} \\mathbf{s}^{(t)}}\n\\end{aligned}\n\\]\nL-BFGS (Limited-memory BFGS): Stores only last m vectors to approximate inverse Hessian"
  },
  {
    "objectID": "Session5/index.html#constrained-optimization",
    "href": "Session5/index.html#constrained-optimization",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nLagrange Multipliers\nFor problems with constraints, we use Lagrange multipliers:\n\\[\n\\mathcal{L}(\\mathbf{w}, \\lambda) = f(\\mathbf{w}) + \\lambda^T g(\\mathbf{w})\n\\]\nwhere \\(g(\\mathbf{w}) \\leq 0\\) are the constraints.\n\n\nKarush-Kuhn-Tucker (KKT) Conditions\nThe necessary conditions for optimality in constrained optimization:\n\nStationarity: \\(\\nabla f(\\mathbf{w}^*) + \\sum \\lambda_i \\nabla g_i(\\mathbf{w}^*) = 0\\)\nPrimal feasibility: \\(g_i(\\mathbf{w}^*) \\leq 0\\)\nDual feasibility: \\(\\lambda_i \\geq 0\\)\nComplementary slackness: \\(\\lambda_i g_i(\\mathbf{w}^*) = 0\\)"
  },
  {
    "objectID": "Session5/index.html#regularization-and-constrained-formulations",
    "href": "Session5/index.html#regularization-and-constrained-formulations",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Regularization and Constrained Formulations",
    "text": "Regularization and Constrained Formulations\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nL1 and L2 Regularization\nRegularization can be viewed as constrained optimization:\nL2 Regularization (Ridge): \\[\n\\min_{\\mathbf{w}} f(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_2^2\n\\] Equivalent to: \\(\\min_{\\mathbf{w}} f(\\mathbf{w})\\) subject to \\(\\|\\mathbf{w}\\|_2^2 \\leq C\\)\nL1 Regularization (Lasso): \\[\n\\min_{\\mathbf{w}} f(\\mathbf{w}) + \\lambda \\|\\mathbf{w}\\|_1\n\\] Equivalent to: \\(\\min_{\\mathbf{w}} f(\\mathbf{w})\\) subject to \\(\\|\\mathbf{w}\\|_1 \\leq C\\)\n\n\nProximal Operators\nFor non-smooth optimization problems like L1 regularization:\n\\[\n\\text{prox}_{\\lambda h}(\\mathbf{w}) = \\arg\\min_{\\mathbf{z}} \\left\\{ h(\\mathbf{z}) + \\frac{1}{2\\lambda} \\|\\mathbf{z} - \\mathbf{w}\\|_2^2 \\right\\}\n\\]\nFor L1 regularization: \\(\\text{prox}_{\\lambda \\|\\cdot\\|_1}(\\mathbf{w}) = \\text{sign}(\\mathbf{w}) \\max(|\\mathbf{w}| - \\lambda, 0)\\)"
  },
  {
    "objectID": "Session5/index.html#global-optimization-methods",
    "href": "Session5/index.html#global-optimization-methods",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Global Optimization Methods",
    "text": "Global Optimization Methods\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nWhen Local Methods Fail\nFor highly non-convex problems, we need global optimization strategies:\nSimulated Annealing:\n\nInspired by metallurgical annealing\nOccasionally accepts worse solutions to escape local minima\nTemperature parameter controls exploration vs exploitation\n\nGenetic Algorithms:\n\nPopulation-based search\nUses crossover and mutation operators\nMaintains diversity to explore search space\n\nBayesian Optimization:\n\nBuilds probabilistic model of objective function\nUses acquisition function to decide where to sample next\nParticularly useful for expensive black-box functions"
  },
  {
    "objectID": "Session5/index.html#hyperparameter-optimization",
    "href": "Session5/index.html#hyperparameter-optimization",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nThe Outer Optimization Loop\nWhile model parameters are learned via gradient descent, hyperparameters require different strategies:\n\nGrid Search: Exhaustive search over predefined hyperparameter grid\nRandom Search: More efficient than grid search for high dimensions\nBayesian Optimization: Adaptive sampling based on previous results\n\n\n\nMulti-Fidelity Optimization\nUsing cheaper approximations to guide search:\n\nLearning curve extrapolation: Predict final performance from early training\nSuccessive halving: Allocate more resources to promising configurations\nHyperband: Adaptive resource allocation strategy"
  },
  {
    "objectID": "Session5/index.html#optimization-in-deep-learning",
    "href": "Session5/index.html#optimization-in-deep-learning",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Optimization in Deep Learning",
    "text": "Optimization in Deep Learning\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nChallenges in Deep Learning Optimization\nDeep neural networks present unique optimization challenges:\nVanishing/Exploding Gradients:\n\nGradients become extremely small or large through many layers\nSolved with careful initialization, batch normalization, residual connections\n\nSaddle Points:\n\nMore problematic than local minima in high dimensions\nSecond-order methods can help identify and escape\n\nIll-conditioning:\n\nHessian matrix has large condition number\nAdaptive methods like Adam help mitigate this\n\n\n\nOptimization for Specific Architectures\n\nCNNs: Spatial structure enables efficient convolutional operations\nRNNs: Backpropagation through time, gradient clipping for stability\nTransformers: Careful initialization, learning rate warmup"
  },
  {
    "objectID": "Session5/index.html#teaching-optimization-with-intuition",
    "href": "Session5/index.html#teaching-optimization-with-intuition",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Teaching Optimization with Intuition",
    "text": "Teaching Optimization with Intuition\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nThe “Mountain Descent” Analogy for Optimization Methods\n\nGradient Descent: Like walking straight downhill, following the steepest slope\nMomentum: Like a ball rolling downhill - builds speed in consistent directions\nAdam: Like an experienced hiker with different step sizes for different terrains\nNewton’s Method: Like using a topographic map to find the valley directly\n\n\n\nThe “Restaurant Menu” Analogy for Regularization\nThink of model complexity like a restaurant menu:\n\nNo regularization: Huge menu with every possible dish (overfits to customer preferences)\nL2 regularization: Reasonable-sized menu with popular dishes\nL1 regularization: Small menu with only essential, popular dishes (feature selection)\n\n\n\nThe “Resource Allocation” Analogy for Hyperparameter Optimization\nLike a venture capitalist funding startups:\n\nGrid search: Fund every possible combination equally\nRandom search: Fund random combinations, see which show promise\nBayesian optimization: Learn from previous investments, fund most promising directions"
  },
  {
    "objectID": "Session5/index.html#convergence-analysis",
    "href": "Session5/index.html#convergence-analysis",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Convergence Analysis",
    "text": "Convergence Analysis\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nRates of Convergence\nDifferent algorithms have different convergence guarantees:\n\nGradient Descent: \\(O(1/\\epsilon)\\) iterations for \\(\\epsilon\\)-accuracy on smooth convex functions\nAccelerated Gradient: \\(O(1/\\sqrt{\\epsilon})\\) iterations\nNewton’s Method: Quadratic convergence near optimum\n\n\n\nPractical Convergence Diagnostics\nIn practice, we monitor:\n\nLoss curve: Should decrease and stabilize\nGradient norm: Should approach zero\nParameter changes: Should become small\nValidation performance: Should improve then stabilize"
  },
  {
    "objectID": "Session5/index.html#common-student-challenges-and-solutions",
    "href": "Session5/index.html#common-student-challenges-and-solutions",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\n\n\n\n\n\nCaution\n\n\n\n\n\n\n\nChallenge 1: “Which optimizer should I use?”\nSolution: Provide a decision framework:\n\nStart with Adam for most problems (robust, little tuning needed)\nUse SGD with momentum if you need the best possible performance (with careful tuning)\nConsider second-order methods for small-to-medium convex problems\nUse specialized optimizers for specific architectures\n\n\n\nChallenge 2: “How do I set the learning rate?”\nSolution: Recommend strategies:\n\nUse learning rate finder (train with increasing learning rates, find where loss decreases fastest)\nUse cyclical learning rates\nStart with common defaults (Adam: 0.001, SGD: 0.1) and adjust based on loss curve\nUse learning rate scheduling (step decay, cosine annealing)\n\n\n\nChallenge 3: “My model isn’t converging - what’s wrong?”\nSolution: Systematic debugging:\n\nCheck gradient flow (vanishing/exploding gradients)\nVerify data pipeline (shuffling, normalization)\nTry overfitting a small batch (if can’t overfit, model is too small or data)\nMonitor gradient norms and parameter updates\nCheck for implementation bugs"
  },
  {
    "objectID": "Session5/index.html#hands-on-exploration-optimization-in-practice",
    "href": "Session5/index.html#hands-on-exploration-optimization-in-practice",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Hands-On Exploration: Optimization in Practice",
    "text": "Hands-On Exploration: Optimization in Practice\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nComparing Optimization Algorithms\nImplement and compare on Iris classification:\n\nSGD with and without momentum\nAdam, RMSProp, AdaGrad\nVisualize optimization paths in 2D parameter space\nCompare convergence rates and final performance\n\n\n\nHyperparameter Optimization Demo\nSet up Bayesian optimization for a neural network:\n\nDefine search space for learning rate, hidden units, regularization\nUse tools like Optuna or Hyperopt\nCompare with random search and grid search\nAnalyze trade-off between computation time and performance\n\n\n\nRegularization Effects Visualization\nShow how different regularization strategies affect:\n\nDecision boundaries\nFeature weights\nGeneralization performance\nModel simplicity vs complexity trade-off"
  },
  {
    "objectID": "Session5/index.html#mission-mastering-optimization-strategies",
    "href": "Session5/index.html#mission-mastering-optimization-strategies",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Mission: Mastering Optimization Strategies",
    "text": "Mission: Mastering Optimization Strategies\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nAssignment 1: The Optimization Explorer\nImplement three different optimization algorithms from scratch:\n\nVanilla gradient descent\nMomentum\nAdam\n\nApply them to logistic regression on Iris data and:\n\nPlot convergence curves\nVisualize optimization paths\nCompare sensitivity to learning rate\nAnalyze when each method works best\n\n\n\nAssignment 2: The Regularization Analyst\nCompare L1, L2, and elastic net regularization:\n\nImplement proximal gradient descent for L1\nAnalyze effect on feature selection\nCompare generalization performance\nCreate visualizations of regularization paths\n\n\n\nAssignment 3: The Hyperparameter Strategist\nOptimize hyperparameters for a neural network using:\n\nGrid search\nRandom search\n\nBayesian optimization\n\nCompare:\n\nTime to find good configurations\nBest performance found\nResource requirements\nPractical recommendations for different scenarios"
  },
  {
    "objectID": "Session5/slides-ML.html#the-art-and-science-of-finding-the-best",
    "href": "Session5/slides-ML.html#the-art-and-science-of-finding-the-best",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "The Art and Science of Finding the Best",
    "text": "The Art and Science of Finding the Best"
  },
  {
    "objectID": "Session5/slides-ML.html#the-optimization-framework",
    "href": "Session5/slides-ML.html#the-optimization-framework",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "The Optimization Framework",
    "text": "The Optimization Framework\n\n\n\n\n\n\nNote\n\n\n\nMost ML problems can be modelled as optimization problems\nUltimate aim is to minimize the error in decision\nConvex and non-convex optimization\nBasic ideas from calculus based optimization"
  },
  {
    "objectID": "Session5/slides-ML.html#first-order-methods-following-the-gradient",
    "href": "Session5/slides-ML.html#first-order-methods-following-the-gradient",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "First-Order Methods: Following the Gradient",
    "text": "First-Order Methods: Following the Gradient\n\n\n\n\n\n\nNote\n\n\n\nGradient as a best direction of increase/ decrease\nGradient descent method as a conqure and win minimization model\nVariants of Gradient descend method"
  },
  {
    "objectID": "Session5/slides-ML.html#second-order-methods-using-curvature-information",
    "href": "Session5/slides-ML.html#second-order-methods-using-curvature-information",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Second-Order Methods: Using Curvature Information",
    "text": "Second-Order Methods: Using Curvature Information\n\n\n\n\n\n\nNote\n\n\n\nBased on second derivative of the loss function\nClassical methods\n\nNewton’s method\nQuasi-Newton’s method"
  },
  {
    "objectID": "Session5/slides-ML.html#constrained-optimization",
    "href": "Session5/slides-ML.html#constrained-optimization",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Constrained Optimization",
    "text": "Constrained Optimization\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#regularization-and-constrained-formulations",
    "href": "Session5/slides-ML.html#regularization-and-constrained-formulations",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Regularization and Constrained Formulations",
    "text": "Regularization and Constrained Formulations\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#global-optimization-methods",
    "href": "Session5/slides-ML.html#global-optimization-methods",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Global Optimization Methods",
    "text": "Global Optimization Methods\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#hyperparameter-optimization",
    "href": "Session5/slides-ML.html#hyperparameter-optimization",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Hyperparameter Optimization",
    "text": "Hyperparameter Optimization\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#optimization-in-deep-learning",
    "href": "Session5/slides-ML.html#optimization-in-deep-learning",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Optimization in Deep Learning",
    "text": "Optimization in Deep Learning\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#teaching-optimization-with-intuition",
    "href": "Session5/slides-ML.html#teaching-optimization-with-intuition",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Teaching Optimization with Intuition",
    "text": "Teaching Optimization with Intuition\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#convergence-analysis",
    "href": "Session5/slides-ML.html#convergence-analysis",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Convergence Analysis",
    "text": "Convergence Analysis\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#common-student-challenges-and-solutions",
    "href": "Session5/slides-ML.html#common-student-challenges-and-solutions",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\n\n\n\n\n\nCaution"
  },
  {
    "objectID": "Session5/slides-ML.html#hands-on-exploration-optimization-in-practice",
    "href": "Session5/slides-ML.html#hands-on-exploration-optimization-in-practice",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Hands-On Exploration: Optimization in Practice",
    "text": "Hands-On Exploration: Optimization in Practice\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session5/slides-ML.html#mission-mastering-optimization-strategies",
    "href": "Session5/slides-ML.html#mission-mastering-optimization-strategies",
    "title": "Session 5: Optimization - The Strategy of Search",
    "section": "Mission: Mastering Optimization Strategies",
    "text": "Mission: Mastering Optimization Strategies\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "Session3/index.html#the-mathematics-of-change-and-improvement",
    "href": "Session3/index.html#the-mathematics-of-change-and-improvement",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Mathematics of Change and Improvement",
    "text": "The Mathematics of Change and Improvement\n\n\n\n\n\n\nCalculus will serve as:\n\n\n\n\nTool that makes learning possible\nTransform static models to dynamic learner\n\n\n\nWelcome to our exploration of calculus, the second pillar of artificial intelligence that provides the fundamental mechanism for learning and adaptation. If linear algebra gives us the language to describe intelligent systems, then calculus gives us the tools to make them learn and improve. Calculus is the mathematical engine that transforms static models into dynamic learners, turning data into intelligence through the principled mathematics of change.\nIn this session, we will uncover how derivatives, gradients, and the chain rule enable machines to learn from their mistakes, optimize their performance, and adapt to new information. We will see how centuries-old mathematical concepts provide the foundation for the most advanced learning algorithms of our time."
  },
  {
    "objectID": "Session3/index.html#the-derivative-measuring-sensitivity-and-change",
    "href": "Session3/index.html#the-derivative-measuring-sensitivity-and-change",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Derivative: Measuring Sensitivity and Change",
    "text": "The Derivative: Measuring Sensitivity and Change\n\nInstantaneous rate of change\nMeasure of rate of increase/ decrease of function\nProperties\nCritcial points and its meaning\n\n\nThe Foundation: Instantaneous Rate of Change\nThe derivative of a function at a point measures how sensitive the output is to small changes in the input:\n\\[\nf'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n\\]\nIn machine learning, this concept becomes profoundly practical. For a model with parameters \\(\\mathbf{w}\\) and loss function \\(\\mathcal{L}(\\mathbf{w})\\), the partial derivative:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial w_i}\n\\]\nmeasures how much the loss would change if we made a small adjustment to parameter \\(w_i\\). This simple concept becomes the guiding compass for model improvement.\n\n\nPartial Derivatives in High Dimensions\nIn our Iris classification problem, we might have a loss function that depends on multiple parameters. For a linear classifier:\n\\[\n\\mathcal{L}(\\mathbf{w}) = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{w}^T\\mathbf{x}_i)^2\n\\]\nEach partial derivative \\(\\frac{\\partial \\mathcal{L}}{\\partial w_j}\\) tells us how changing weight \\(w_j\\) affects the overall classification error. The collection of all these partial derivatives forms the gradient."
  },
  {
    "objectID": "Session3/index.html#the-gradient-the-learning-compass",
    "href": "Session3/index.html#the-gradient-the-learning-compass",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Gradient: The Learning Compass",
    "text": "The Gradient: The Learning Compass\n\nThe direction of maximum increase\nOrtogonal direction to the surface landscape\n\n\nThe Gradient Vector\nThe gradient collects all partial derivatives into a single vector:\n\\[\n\\nabla\\mathcal{L}(\\mathbf{w}) = \\begin{bmatrix}\n\\frac{\\partial \\mathcal{L}}{\\partial w_1} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_2} \\\\\n\\vdots \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial w_d}\n\\end{bmatrix}\n\\]\nThis vector points in the direction of steepest ascent of the loss function. Its magnitude tells us how steep the slope is in that direction. For learning, we’re interested in the opposite direction—the direction of steepest descent.\n\n\nGeometric Interpretation: Navigating Loss Landscapes\nImagine our loss function as a mountainous landscape. The height at each point represents how bad our model performs with particular parameters. We start at a random point in this landscape and want to find the lowest valley.\nThe gradient at our current position tells us which direction is uphill. Therefore, \\(-\\nabla\\mathcal{L}(\\mathbf{w})\\) points downhill—the direction we should move to reduce our loss. This geometric intuition transforms abstract mathematics into a concrete navigation strategy."
  },
  {
    "objectID": "Session3/index.html#gradient-descent-the-learning-algorithm",
    "href": "Session3/index.html#gradient-descent-the-learning-algorithm",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Gradient Descent: The Learning Algorithm",
    "text": "Gradient Descent: The Learning Algorithm\n\nIterative algorithm to global minimum\nSimple definition: \\(\\theta^{n+1}=\\theta^n-\\eta \\nabla \\mathcal{L}(\\theta^n)\\)\nDifferent types of Grradient Descent\nThe first algorithm to minimize loss in classical ML\n\n\nThe Update Rule\nGradient descent follows a simple iterative process:\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla\\mathcal{L}(\\mathbf{w}^{(t)})\n\\]\nwhere \\(\\eta\\) is the learning rate. This elegant equation embodies the entire concept of learning through gradual improvement.\n\n\nThe Learning Rate: Step Size Matters\nThe learning rate \\(\\eta\\) controls how far we move in the gradient direction. Its choice is crucial:\n\nToo small: Learning is slow, may get stuck in local minima\nToo large: May overshoot the minimum, causing oscillation or divergence\nJust right: Efficient convergence to a good solution\n\n\n\nVisualizing Gradient Descent\nFor our Iris classifier, we can visualize gradient descent as a path through parameter space. The algorithm starts with random weights and gradually moves toward regions where the classifier makes fewer mistakes. Each step is determined by the local gradient, creating a path that follows the topography of the loss landscape."
  },
  {
    "objectID": "Session3/index.html#the-chain-rule-the-foundation-of-deep-learning",
    "href": "Session3/index.html#the-chain-rule-the-foundation-of-deep-learning",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Chain Rule: The Foundation of Deep Learning",
    "text": "The Chain Rule: The Foundation of Deep Learning\n\nMeasure the combosit change\nThe workhorse of back propagation in ML\n\n\nComposing Functions\nThe chain rule tells us how to differentiate composite functions:\n\\[\n\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\n\\]\nIn the context of neural networks, this becomes essential for computing gradients through multiple layers.\n\n\nBackpropagation: Applying the Chain Rule to Networks\nConsider a simple neural network with one hidden layer:\n\\[\n\\begin{aligned}\n\\mathbf{z} &= \\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1 \\\\\n\\mathbf{h} &= \\sigma(\\mathbf{z}) \\\\\n\\mathbf{o} &= \\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2 \\\\\n\\mathcal{L} &= \\text{loss}(\\mathbf{o}, \\mathbf{y})\n\\end{aligned}\n\\]\nBackpropagation uses the chain rule to compute gradients for all parameters:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_2} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}} \\cdot \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}_2}\n\\]\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{o}} \\cdot \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}} \\cdot \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}_1}\n\\]\nThis systematic application of the chain rule allows gradients to flow backward through the network, enabling learning in deep architectures."
  },
  {
    "objectID": "Session3/index.html#loss-functions-quantifying-what-we-want-to-improve",
    "href": "Session3/index.html#loss-functions-quantifying-what-we-want-to-improve",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Loss Functions: Quantifying What We Want to Improve",
    "text": "Loss Functions: Quantifying What We Want to Improve\n\nLoss function or cost function as the cost of approaximation\nFormation of loss function\nDifferent types of loss functions\n\n\nCommon Loss Functions\nDifferent problems require different ways of measuring “wrongness”:\nMean Squared Error (Regression): \\[\n\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nCross-Entropy Loss (Classification): \\[\n\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{c=1}^C y_{i,c} \\log(\\hat{y}_{i,c})\n\\]\nHinge Loss (SVMs): \\[\n\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n \\max(0, 1 - y_i(\\mathbf{w}^T\\mathbf{x}_i))\n\\]\nEach loss function has different gradient properties that affect how learning proceeds.\n\n\nConvex vs Non-Convex Loss Landscapes\n\nConvex functions (like linear regression): Have one global minimum, guaranteed convergence\nNon-convex functions (like neural networks): Have multiple local minima, convergence to global minimum not guaranteed\n\nUnderstanding the loss landscape helps us choose appropriate optimization strategies."
  },
  {
    "objectID": "Session3/index.html#advanced-optimization-algorithms",
    "href": "Session3/index.html#advanced-optimization-algorithms",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Advanced Optimization Algorithms",
    "text": "Advanced Optimization Algorithms\n\nBetter optimize the loss with experience of gradients\nModern optimizors\n\nmomentum\n\nAdaptive family\n\nRMSProp\nAdagrad\nAdam\nAdamw\n\n\n\nMomentum: Learning with Inertia\nMomentum adds a velocity term to gradient descent:\n\\[\n\\begin{aligned}\n\\mathbf{v}^{(t+1)} &= \\gamma \\mathbf{v}^{(t)} + \\eta \\nabla\\mathcal{L}(\\mathbf{w}^{(t)}) \\\\\n\\mathbf{w}^{(t+1)} &= \\mathbf{w}^{(t)} - \\mathbf{v}^{(t+1)}\n\\end{aligned}\n\\]\nThis helps smooth out oscillations and navigate flat regions more efficiently.\n\n\nAdaptive Learning Rates\nAlgorithms like Adam, RMSProp, and Adagrad adapt the learning rate for each parameter based on historical gradient information:\n\\[\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\frac{\\eta}{\\sqrt{\\hat{\\mathbf{v}}^{(t)}} + \\epsilon} \\hat{\\mathbf{m}}^{(t)}\n\\]\nThese methods often converge faster and require less tuning of the learning rate."
  },
  {
    "objectID": "Session3/index.html#teaching-calculus-with-intuition",
    "href": "Session3/index.html#teaching-calculus-with-intuition",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Teaching Calculus with Intuition",
    "text": "Teaching Calculus with Intuition\n\nUse anlogies\nIdentiy the best hack to introduce tough ideas like back propagation\n\n\nThe “Hiker in the Fog” Analogy for Gradient Descent\nImagine you’re a hiker in thick fog, trying to find the valley below. You can’t see the entire landscape, but you can feel the slope beneath your feet. The gradient is this “feeling of the slope.” Each step you take downhill is like a gradient descent update. The learning rate is how big your steps are—too small and you’ll take forever, too big and you might overshoot the valley.\n\n\nThe “Factory Assembly Line” for Backpropagation\nThink of a neural network as a factory assembly line. Raw materials (input data) enter at one end, and each station (layer) transforms them. When the final product (prediction) has defects, we need to figure out which stations caused the problems. Backpropagation is like sending quality control reports backward through the factory—each station learns how to adjust its process based on its contribution to the final error.\n\n\nThe “Thermostat” Analogy for Learning Rate\nThe learning rate is like a thermostat control. If the room is too cold, you might turn up the heat a lot (high learning rate). But if you overshoot and it gets too hot, you might need to make smaller adjustments (low learning rate). Some advanced thermostats learn from past adjustments, just like adaptive learning rate algorithms."
  },
  {
    "objectID": "Session3/index.html#calculus-in-modern-architectures",
    "href": "Session3/index.html#calculus-in-modern-architectures",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Calculus in Modern Architectures",
    "text": "Calculus in Modern Architectures\n\n\\(A(Q,K,V)\\)- the attention gate of gradient contribution\nGradient in Min-Max game like losses in GAN\nGradient aware activations\nGradent aware coposit losses\n\n\nAttention Mechanisms and Gradients\nIn transformer models, the attention mechanism computes:\n\\[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\n\\]\nThe gradients flowing through this operation allow the model to learn which parts of the input to “pay attention to” for different tasks.\n\n\nGenerative Models and Gradient Flow\nIn Generative Adversarial Networks (GANs), we have two networks—a generator and a discriminator—playing a minimax game:\n\\[\n\\min_G \\max_D \\mathbb{E}[\\log D(\\mathbf{x})] + \\mathbb{E}[\\log(1 - D(G(\\mathbf{z})))]\n\\]\nThe careful balance of gradients between these competing networks enables the generation of realistic data."
  },
  {
    "objectID": "Session3/index.html#common-student-challenges-and-solutions",
    "href": "Session3/index.html#common-student-challenges-and-solutions",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\nWhy calculus in AI?\nWhy do we learn chain rule if we have functions to calculate gradient?\nHow can I confirm a minimum point as local or global?\n\n\nChallenge 1: “Why do we need calculus? Can’t we just try different parameters randomly?”\nSolution: Use the “searching for keys” analogy. Searching randomly in a dark room for your keys might eventually work, but it’s incredibly inefficient. Using gradients is like having a metal detector that beeps louder as you get closer—it gives you directional information that dramatically speeds up the search.\n\n\nChallenge 2: “The chain rule seems complicated with all these partial derivatives.”\nSolution: Emphasize that backpropagation is just the systematic application of the chain rule. Draw the computational graph and show how errors flow backward. Start with simple networks and gradually increase complexity.\n\n\nChallenge 3: “How do we know we’re finding the global minimum and not just a local minimum?”\nSolution: Acknowledge that for non-convex problems, we can’t guarantee finding the global minimum. However, in practice, many local minima in deep learning are “good enough,” and techniques like random initialization, batch normalization, and ensemble methods help us find good solutions."
  },
  {
    "objectID": "Session3/index.html#hands-on-exploration-seeing-learning-happen",
    "href": "Session3/index.html#hands-on-exploration-seeing-learning-happen",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Hands-On Exploration: Seeing Learning Happen",
    "text": "Hands-On Exploration: Seeing Learning Happen\n\nVisualization of learning using the gradient flow\nDemonstrating the role of learning rate by visualizations\n\n\nVisualizing Gradient Descent\nWe can create visualizations that show:\n\nGradient descent paths on 2D loss surfaces\nThe effect of different learning rates\nHow momentum affects the optimization trajectory\nComparison of different optimization algorithms\n\n\n\nMonitoring Training Dynamics\nBy plotting loss curves and gradient norms during training, we can diagnose problems like: - Learning rate too high (oscillating loss) - Learning rate too low (very slow decrease) - Vanishing gradients (gradient norms decreasing too rapidly) - Exploding gradients (gradient norms increasing)"
  },
  {
    "objectID": "Session3/index.html#mission-mastering-the-learning-process",
    "href": "Session3/index.html#mission-mastering-the-learning-process",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Mission: Mastering the Learning Process",
    "text": "Mission: Mastering the Learning Process\n\nDevelop a gradent visualizer for common convex functions\nDesign a simple neural network and develop an easy method to demonstrate the back propagation updates\nCompare and contrast at least three widely used optimizers in ML\n\n\nAssignment 1: The Gradient Explorer\nImplement gradient descent for a simple linear regression problem on the Iris dataset. Create a visualization that shows:\n\nThe loss surface\nThe path taken by gradient descent\nHow the gradient direction changes along the path\nThe effect of different learning rates\n\n\n\nAssignment 2: The Backpropagation Storyteller\nChoose a simple neural network architecture. Create a step-by-step explanation of how backpropagation works, showing:\n\nThe forward pass computations\nThe backward pass gradients\nHow each layer’s parameters get updated\nThe role of the chain rule at each step\n\n\n\nAssignment 3: The Optimization Strategist\nCompare three different optimization algorithms (e.g., SGD, Momentum, Adam) on a classification task. Analyze:\n\nConvergence speed\nFinal performance\nSensitivity to learning rate\nComputational requirements\nWhen you might choose each algorithm"
  },
  {
    "objectID": "Session3/slides-ML.html#the-mathematics-of-change-and-improvement",
    "href": "Session3/slides-ML.html#the-mathematics-of-change-and-improvement",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Mathematics of Change and Improvement",
    "text": "The Mathematics of Change and Improvement\n\n\n\n\n\n\nCalculus will serve as:\n\n\n\nTool that makes learning possible\nTransform static models to dynamic learner"
  },
  {
    "objectID": "Session3/slides-ML.html#the-derivative-measuring-sensitivity-and-change",
    "href": "Session3/slides-ML.html#the-derivative-measuring-sensitivity-and-change",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Derivative: Measuring Sensitivity and Change",
    "text": "The Derivative: Measuring Sensitivity and Change\n\nInstantaneous rate of change\nMeasure of rate of increase/ decrease of function\nProperties\nCritcial points and its meaning"
  },
  {
    "objectID": "Session3/slides-ML.html#the-gradient-the-learning-compass",
    "href": "Session3/slides-ML.html#the-gradient-the-learning-compass",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Gradient: The Learning Compass",
    "text": "The Gradient: The Learning Compass\n\nThe direction of maximum increase\nOrtogonal direction to the surface landscape"
  },
  {
    "objectID": "Session3/slides-ML.html#gradient-descent-the-learning-algorithm",
    "href": "Session3/slides-ML.html#gradient-descent-the-learning-algorithm",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Gradient Descent: The Learning Algorithm",
    "text": "Gradient Descent: The Learning Algorithm\n\nIterative algorithm to global minimum\nSimple definition: \\(\\theta^{n+1}=\\theta^n-\\eta \\nabla \\mathcal{L}(\\theta^n)\\)\nDifferent types of Grradient Descent\nThe first algorithm to minimize loss in classical ML"
  },
  {
    "objectID": "Session3/slides-ML.html#the-chain-rule-the-foundation-of-deep-learning",
    "href": "Session3/slides-ML.html#the-chain-rule-the-foundation-of-deep-learning",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "The Chain Rule: The Foundation of Deep Learning",
    "text": "The Chain Rule: The Foundation of Deep Learning\n\nMeasure the combosit change\nThe workhorse of back propagation in ML"
  },
  {
    "objectID": "Session3/slides-ML.html#loss-functions-quantifying-what-we-want-to-improve",
    "href": "Session3/slides-ML.html#loss-functions-quantifying-what-we-want-to-improve",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Loss Functions: Quantifying What We Want to Improve",
    "text": "Loss Functions: Quantifying What We Want to Improve\n\nLoss function or cost function as the cost of approaximation\nFormation of loss function\nDifferent types of loss functions"
  },
  {
    "objectID": "Session3/slides-ML.html#advanced-optimization-algorithms",
    "href": "Session3/slides-ML.html#advanced-optimization-algorithms",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Advanced Optimization Algorithms",
    "text": "Advanced Optimization Algorithms\n\nBetter optimize the loss with experience of gradients\nModern optimizors\n\nmomentum\n\nAdaptive family\n\nRMSProp\nAdagrad\nAdam\nAdamw"
  },
  {
    "objectID": "Session3/slides-ML.html#teaching-calculus-with-intuition",
    "href": "Session3/slides-ML.html#teaching-calculus-with-intuition",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Teaching Calculus with Intuition",
    "text": "Teaching Calculus with Intuition\n\nUse anlogies\nIdentiy the best hack to introduce tough ideas like back propagation"
  },
  {
    "objectID": "Session3/slides-ML.html#calculus-in-modern-architectures",
    "href": "Session3/slides-ML.html#calculus-in-modern-architectures",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Calculus in Modern Architectures",
    "text": "Calculus in Modern Architectures\n\n\\(A(Q,K,V)\\)- the attention gate of gradient contribution\nGradient in Min-Max game like losses in GAN\nGradient aware activations\nGradent aware coposit losses"
  },
  {
    "objectID": "Session3/slides-ML.html#common-student-challenges-and-solutions",
    "href": "Session3/slides-ML.html#common-student-challenges-and-solutions",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\nWhy calculus in AI?\nWhy do we learn chain rule if we have functions to calculate gradient?\nHow can I confirm a minimum point as local or global?"
  },
  {
    "objectID": "Session3/slides-ML.html#hands-on-exploration-seeing-learning-happen",
    "href": "Session3/slides-ML.html#hands-on-exploration-seeing-learning-happen",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Hands-On Exploration: Seeing Learning Happen",
    "text": "Hands-On Exploration: Seeing Learning Happen\n\nVisualization of learning using the gradient flow\nDemonstrating the role of learning rate by visualizations"
  },
  {
    "objectID": "Session3/slides-ML.html#mission-mastering-the-learning-process",
    "href": "Session3/slides-ML.html#mission-mastering-the-learning-process",
    "title": "Session 3: Calculus - The Engine of Learning",
    "section": "Mission: Mastering the Learning Process",
    "text": "Mission: Mastering the Learning Process\n\nDevelop a gradent visualizer for common convex functions\nDesign a simple neural network and develop an easy method to demonstrate the back propagation updates\nCompare and contrast at least three widely used optimizers in ML"
  },
  {
    "objectID": "Session1/index.html#key-points",
    "href": "Session1/index.html#key-points",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Key points",
    "text": "Key points\n\nArchitectural Vision of the Course\nFour Pillers of AI\nThe Linear Algebra Perspective: Data as Mathematical Objects\nThe Statistical Foundation: Measuring and Modeling Uncertainty\nThe Calculus Engine: The Mathematics of Improvement\nThe Optimization Process: The Journey to Better Solutions"
  },
  {
    "objectID": "Session1/index.html#the-architectural-vision-of-artificial-intelligence",
    "href": "Session1/index.html#the-architectural-vision-of-artificial-intelligence",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "The Architectural Vision of Artificial Intelligence",
    "text": "The Architectural Vision of Artificial Intelligence\nWelcome to our journey into the fundamental architecture of artificial intelligence and machine learning. Today, we lay the groundwork for understanding not just how AI algorithms work, but why they work. We begin by exploring the four mathematical pillars that support every artificial intelligence system, from the simplest linear regression to the most complex large language model.\nImagine constructing a magnificent cathedral. Before the first stone is laid, the architects must understand the fundamental principles of physics, materials science, and structural engineering. Similarly, before we can truly understand artificial intelligence, we must grasp the mathematical principles that make it possible. These principles are organized into four interconnected disciplines that form the foundation of all AI systems."
  },
  {
    "objectID": "Session1/index.html#the-four-pillars-a-cohesive-framework",
    "href": "Session1/index.html#the-four-pillars-a-cohesive-framework",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "The Four Pillars: A Cohesive Framework",
    "text": "The Four Pillars: A Cohesive Framework\nThe edifice of artificial intelligence stands firmly on four mathematical pillars: Linear Algebra, Calculus, Probability and Statistics, and Optimization. These are not isolated domains of knowledge but rather interconnected disciplines that work in concert to create intelligent systems.\nLinear Algebra provides the language and structure for representing data and transformations. It gives us the vocabulary to describe the world in terms of vectors, matrices, and linear transformations. When we work with images, text, or any form of data, we are essentially working with multidimensional mathematical objects that linear algebra helps us manipulate and understand.\nCalculus serves as the engine of learning in artificial intelligence. It provides the mechanisms for understanding change and sensitivity through derivatives and gradients. Just as a hiker navigates unfamiliar terrain by sensing the slope beneath their feet, machine learning algorithms navigate complex mathematical landscapes by following gradients toward optimal solutions.\nProbability and Statistics form the science of uncertainty and inference. In a world filled with noisy data and imperfect measurements, these disciplines provide the tools for reasoning under uncertainty, making inferences from limited information, and quantifying the reliability of our predictions. They transform raw data into meaningful patterns and insights.\nOptimization represents the art and science of finding the best possible solutions. It provides the strategies and algorithms for systematically improving models and converging toward optimal configurations. Where calculus tells us which direction to move, optimization tells us how far to step and when we have arrived at a good enough solution.\nTogether, these four pillars create a complete framework for building, understanding, and improving artificial intelligence systems."
  },
  {
    "objectID": "Session1/index.html#deconstructing-a-simple-model-linear-regression-through-the-four-lenses",
    "href": "Session1/index.html#deconstructing-a-simple-model-linear-regression-through-the-four-lenses",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Deconstructing a Simple Model: Linear Regression Through the Four Lenses",
    "text": "Deconstructing a Simple Model: Linear Regression Through the Four Lenses\n\nData as Mathematical Objects\nMeasuring and Modeling Uncertainty\nThe Mathematics of Improvement\nThe Journey to Better Solutions\n\n\nLinear Regression Through the Four Lenses\nLet us examine how these four pillars work together by analyzing one of the simplest machine learning algorithms: linear regression. This deconstruction will reveal the hidden mathematical structure that powers even the most sophisticated AI systems.\nThe Linear Algebra Perspective: Data as Mathematical Objects Linear algebra transforms our understanding of data from mere numbers to structured mathematical objects. In linear regression, we represent our input features as a design matrix where each row corresponds to an observation and each column to a feature. The target values become a vector in a high-dimensional space.\nThe fundamental operation of linear regression—making predictions—is essentially a linear transformation. The model parameters form a weight vector that, when multiplied with the input matrix, produces predictions. This matrix-vector multiplication represents a sophisticated way of combining features through weighted sums, demonstrating how linear algebra provides the mathematical language for data representation and transformation.\nThe Statistical Foundation: Measuring and Modeling Uncertainty Statistics provides the framework for understanding what makes a model “good” or “bad.” The core of linear regression from a statistical perspective is the concept of a loss function—specifically, the mean squared error. This function measures the average squared difference between our predictions and the actual values, giving us a quantitative way to assess model performance.\nBeyond mere error measurement, statistics helps us understand the assumptions behind our models. Linear regression assumes that errors are normally distributed and that relationships between variables are linear. Statistical inference allows us to test these assumptions, compute confidence intervals for our predictions, and understand the limitations of our models in real-world applications.\nThe Calculus Engine: Calculus provides the mechanism for learning and improvement through the concept of gradients. The gradient of the loss function with respect to the model parameters tells us the direction of steepest ascent—or, if we reverse it, the direction of steepest descent. Each component of this gradient vector indicates how much the loss would change if we made a small adjustment to the corresponding parameter.\nThis mathematical insight transforms the problem of finding optimal parameters from a guessing game into a systematic process. By repeatedly computing gradients and taking small steps in the negative gradient direction, we can navigate the complex landscape of possible models toward regions of better performance. This process, known as gradient descent, exemplifies how calculus powers the learning in machine learning.\nThe Optimization Process: Optimization provides the strategy and methodology for the learning journey. While calculus gives us the direction to move, optimization determines how far we should step (the learning rate), when we should stop (convergence criteria), and how to handle obstacles like local minima.\nIn linear regression, the optimization process involves iteratively updating parameters based on gradient information until we reach a point where further improvements become negligible. Optimization theory helps us choose appropriate learning rates, implement momentum to navigate flat regions, and design stopping conditions that balance computation time with model quality."
  },
  {
    "objectID": "Session1/index.html#mapping-our-curriculum-to-the-four-pillars",
    "href": "Session1/index.html#mapping-our-curriculum-to-the-four-pillars",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Mapping Our Curriculum to the Four Pillars",
    "text": "Mapping Our Curriculum to the Four Pillars\n\nCore Machine Learning module draws equally from all four pillars\nThe Deep Learning module places particular emphasis on linear algebra and calculus\nBayesian Methods highlights the power of probability and statistics\nOptimization Theory and Cloud Deployment complete the picture by focusing on the practical aspects of making AI work in real-world scenarios\n\nAs we progress through this course, you will discover that every topic, every algorithm, and every technique connects back to these four fundamental pillars. Let us preview how different modules of our curriculum emphasize different aspects of this mathematical foundation.\nOur Core Machine Learning module draws equally from all four pillars. Supervised learning algorithms like logistic regression and support vector machines require linear algebra for feature representation, calculus for gradient computation, statistics for model evaluation, and optimization for parameter tuning. Unsupervised methods like clustering and dimensionality reduction similarly integrate all four mathematical disciplines.\nThe Deep Learning module places particular emphasis on linear algebra and calculus. Neural networks are essentially chains of linear transformations followed by nonlinear activations—a concept deeply rooted in linear algebra. The backpropagation algorithm that trains these networks is an elegant application of the chain rule from calculus, allowing gradients to flow backward through multiple layers of computation.\nOur exploration of Bayesian Methods highlights the power of probability and statistics. Bayesian inference provides a coherent framework for updating beliefs in light of new evidence, while probabilistic programming allows us to build models that explicitly account for uncertainty. These approaches demonstrate how statistical thinking enables more robust and interpretable AI systems.\nFinally, our studies in Optimization Theory and Cloud Deployment complete the picture by focusing on the practical aspects of making AI work in real-world scenarios. From hyperparameter tuning to distributed training, these advanced topics show how optimization principles scale from simple algorithms to enterprise-level systems."
  },
  {
    "objectID": "Session1/index.html#the-art-of-explanation",
    "href": "Session1/index.html#the-art-of-explanation",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "The Art of Explanation",
    "text": "The Art of Explanation\n\nFinding the right analogies and explanations that bridge abstract mathematics and concrete understanding\nIntuitive illustration of Linear Algebra\nCalculus as a tool for analysis\nProbability as a refinement of belief\nOptimization as a quest for better -not the best"
  },
  {
    "objectID": "Session1/index.html#making-mathematics-accessible",
    "href": "Session1/index.html#making-mathematics-accessible",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Making Mathematics Accessible",
    "text": "Making Mathematics Accessible\nOur responsibility as educators extends beyond understanding these mathematical foundations ourselves—we must make them accessible and intuitive for our students. The key to effective teaching lies in finding the right analogies and explanations that bridge abstract mathematics and concrete understanding.\nWhen explaining linear algebra, we might compare vectors to arrows with direction and magnitude, and matrices as machines that stretch, rotate, or skew space. The concept of eigenvectors becomes more intuitive when described as “natural directions” in data that remain unchanged by certain transformations.\nFor calculus, the gradient descent algorithm can be visualized as a hiker trying to find the valley in thick fog by feeling the slope beneath their feet. Each step is determined by the steepness of the terrain, with smaller steps on steeper slopes to avoid overshooting the target.\nProbability concepts come alive when connected to everyday experiences. Bayesian updating can be explained as the process of revising our beliefs about a friend’s punctuality based on whether they arrive on time for meetings. Each observation either strengthens or weakens our initial assessment.\nOptimization strategies can be framed as resource allocation problems. Just as a project manager must allocate limited time and budget across competing tasks, optimization algorithms must allocate “improvement effort” across different parameters to achieve the best overall results."
  },
  {
    "objectID": "Session1/index.html#this-weeks-mission",
    "href": "Session1/index.html#this-weeks-mission",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "This Week’s Mission",
    "text": "This Week’s Mission\n\nSolidify your understanding of these fundamental concepts\nprepare a two-minute “elevator pitch” that explains the four pillars framework to someone with minimal technical background"
  },
  {
    "objectID": "Session1/index.html#building-your-foundation",
    "href": "Session1/index.html#building-your-foundation",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Building Your Foundation",
    "text": "Building Your Foundation\nTo solidify your understanding of these fundamental concepts, your assignment this week focuses on developing both depth of understanding and clarity of explanation.\nFirst, create a comprehensive pillar mapping for the topics you will be teaching. For each major algorithm or technique, identify which mathematical pillars are most relevant and write a brief explanation of how each pillar contributes to the algorithm’s operation. This exercise will help you see the hidden mathematical structure in every AI method you teach.\nSecond, prepare a two-minute “elevator pitch” that explains the four pillars framework to someone with minimal technical background. Your explanation should use a consistent analogy that makes abstract concepts concrete and memorable. Consider comparing the four pillars to the foundation, framework, construction techniques, and finishing work in building a house, or to the ingredients, cooking techniques, taste testing, and recipe refinement in culinary arts.\nThe success of this week will be measured by your ability to see the four pillars in any AI algorithm and to explain their importance in clear, accessible language."
  },
  {
    "objectID": "Session1/slides-quarto.html#key-points",
    "href": "Session1/slides-quarto.html#key-points",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Key points",
    "text": "Key points\n\nArchitectural Vision of the Course\nFour Pillers of AI\nThe Linear Algebra Perspective: Data as Mathematical Objects\nThe Statistical Foundation: Measuring and Modeling Uncertainty\nThe Calculus Engine: The Mathematics of Improvement\nThe Optimization Process: The Journey to Better Solutions"
  },
  {
    "objectID": "Session1/slides-quarto.html#deconstructing-a-simple-model-linear-regression-through-the-four-lenses",
    "href": "Session1/slides-quarto.html#deconstructing-a-simple-model-linear-regression-through-the-four-lenses",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Deconstructing a Simple Model: Linear Regression Through the Four Lenses",
    "text": "Deconstructing a Simple Model: Linear Regression Through the Four Lenses\n\nData as Mathematical Objects\nMeasuring and Modeling Uncertainty\nThe Mathematics of Improvement\nThe Journey to Better Solutions"
  },
  {
    "objectID": "Session1/slides-quarto.html#mapping-our-curriculum-to-the-four-pillars",
    "href": "Session1/slides-quarto.html#mapping-our-curriculum-to-the-four-pillars",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "Mapping Our Curriculum to the Four Pillars",
    "text": "Mapping Our Curriculum to the Four Pillars\n\nCore Machine Learning module draws equally from all four pillars\nThe Deep Learning module places particular emphasis on linear algebra and calculus\nBayesian Methods highlights the power of probability and statistics\nOptimization Theory and Cloud Deployment complete the picture by focusing on the practical aspects of making AI work in real-world scenarios"
  },
  {
    "objectID": "Session1/slides-quarto.html#the-art-of-explanation",
    "href": "Session1/slides-quarto.html#the-art-of-explanation",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "The Art of Explanation",
    "text": "The Art of Explanation\n\nFinding the right analogies and explanations that bridge abstract mathematics and concrete understanding\nIntuitive illustration of Linear Algebra\nCalculus as a tool for analysis\nProbability as a refinement of belief\nOptimization as a quest for better -not the best"
  },
  {
    "objectID": "Session1/slides-quarto.html#this-weeks-mission",
    "href": "Session1/slides-quarto.html#this-weeks-mission",
    "title": "The Foundation Framework - Introducing the Four Pillars of AI/ML",
    "section": "This Week’s Mission",
    "text": "This Week’s Mission\n\nSolidify your understanding of these fundamental concepts\nprepare a two-minute “elevator pitch” that explains the four pillars framework to someone with minimal technical background"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Intelligence",
    "section": "",
    "text": "We are pleased to announce that the ASAP AI/ML Program is hosting a comprehensive Training of Trainers (ToT) series to establish strong mathematical foundations in artificial intelligence and machine learning education.\nArtificial Intelligence and Machine Learning represent the convergence of centuries of mathematical development across multiple disciplines. What appears as technological magic is fundamentally built upon four mathematical pillars: Linear Algebra, Calculus, Probability & Statistics, and Optimization. This expert-led series explores how these foundational disciplines work in concert to create intelligent systems, moving beyond superficial code implementation to deep conceptual understanding. Through interactive demonstrations, mathematical derivations, real-world applications, and pedagogical strategies, we will equip trainers to deliver the revamped AI/ML curriculum with scientific rigor and mathematical integrity.\n\n\n\n\n7-Session ToT Program\nSession 1 — The Foundation Framework - Introduction to Four Mathematical Pillars - Linear Algebra, Calculus, Probability & Statistics, Optimization - Architectural Vision of AI Systems - Computational vs Analytical Mathematics - Assignment: Pillar Mapping & Conceptual Explanations\nSession 2 — Linear Algebra: The Language of Data - Data Representation as Mathematical Objects - Matrix Operations & Geometric Interpretations - Eigenvalues, PCA & Dimensionality Reduction - Teaching Strategies for Abstract Concepts - Hands-On: Matrix Decomposition Visualizations\nSession 3 — Calculus: The Engine of Learning - Derivatives, Gradients & Sensitivity Analysis - Gradient Descent: Theory & Intuition - Backpropagation Demystified - Loss Landscapes & Convergence - Hands-On: Gradient Visualization Tools\nSession 4 — Probability & Statistics: Reasoning with Uncertainty - Statistical Inference & Bayesian Thinking - Probability Distributions in ML - Model Evaluation & Uncertainty Quantification - Statistical Testing & Confidence Intervals - Hands-On: Probabilistic Programming Examples\nSession 5 — Optimization: The Strategy of Search - Convex vs Non-Convex Optimization - Optimization Algorithms & Convergence - Hyperparameter Tuning Strategies - Constraints & Regularization - Hands-On: Optimization Landscape Exploration\nSession 6 — Pillars in Practice: Advanced Architectures - Neural Networks Through Mathematical Lenses - Transformers & Attention Mechanisms - Generative Models & Statistical Physics Connections - Ethical AI & Mathematical Fairness - Case Study: End-to-End Mathematical Deconstruction\nSession 7 — Synthesis & Teaching Excellence - Micro-Teaching Lab - Curriculum Integration Strategies - Assessment Design for Conceptual Understanding - Community Building & Resource Sharing - Final Commitments & Action Plans\n\n\n\nCore Teaching Methodologies\n\nThe 5-Step Framework: Concept → Intuition → Mathematics → Code → Interpretation\nMathematical Storytelling & Conceptual Analogies\nVisual Demonstrations & Geometric Intuition\nHistorical Context & Philosophical Connections\nPedagogical Patterns for Abstract Concepts\n\nTools & Technologies\n\nPython with NumPy, SciPy, Matplotlib\nJupyter Notebook for Interactive Demonstrations\nVisualization Libraries for Mathematical Concepts\nLaTeX for Mathematical Notation\nCollaborative Platforms for Resource Sharing\n\nExpected Trainer Deliverables\n\nComprehensive pillar mapping of assigned curriculum\nIntuitive explanations for complex mathematical concepts\nVisual demonstrations and teaching analogies\nAssessment strategies for conceptual understanding\nLesson plans integrating the four pillars framework\n\nProgram Outcomes\n\nDeliver AI/ML content with strong mathematical foundations\nExplain complex concepts with clarity and intuition\nConnect theoretical mathematics to practical applications\nFoster deep conceptual understanding in students\nCreate a community of mathematically-grounded AI educators\n\n\n\n\n\n\nWe would greatly appreciate it if you could take a few moments to complete our feedback form.\nPlease click the link below or scan the QR code to access the feedback form:\nhttps://forms.gle/xxitNpH2Xr4ogACf6",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#about-the-tot",
    "href": "index.html#about-the-tot",
    "title": "Foundations of Intelligence",
    "section": "",
    "text": "We are pleased to announce that the ASAP AI/ML Program is hosting a comprehensive Training of Trainers (ToT) series to establish strong mathematical foundations in artificial intelligence and machine learning education.\nArtificial Intelligence and Machine Learning represent the convergence of centuries of mathematical development across multiple disciplines. What appears as technological magic is fundamentally built upon four mathematical pillars: Linear Algebra, Calculus, Probability & Statistics, and Optimization. This expert-led series explores how these foundational disciplines work in concert to create intelligent systems, moving beyond superficial code implementation to deep conceptual understanding. Through interactive demonstrations, mathematical derivations, real-world applications, and pedagogical strategies, we will equip trainers to deliver the revamped AI/ML curriculum with scientific rigor and mathematical integrity.\n\n\n\n\n7-Session ToT Program\nSession 1 — The Foundation Framework - Introduction to Four Mathematical Pillars - Linear Algebra, Calculus, Probability & Statistics, Optimization - Architectural Vision of AI Systems - Computational vs Analytical Mathematics - Assignment: Pillar Mapping & Conceptual Explanations\nSession 2 — Linear Algebra: The Language of Data - Data Representation as Mathematical Objects - Matrix Operations & Geometric Interpretations - Eigenvalues, PCA & Dimensionality Reduction - Teaching Strategies for Abstract Concepts - Hands-On: Matrix Decomposition Visualizations\nSession 3 — Calculus: The Engine of Learning - Derivatives, Gradients & Sensitivity Analysis - Gradient Descent: Theory & Intuition - Backpropagation Demystified - Loss Landscapes & Convergence - Hands-On: Gradient Visualization Tools\nSession 4 — Probability & Statistics: Reasoning with Uncertainty - Statistical Inference & Bayesian Thinking - Probability Distributions in ML - Model Evaluation & Uncertainty Quantification - Statistical Testing & Confidence Intervals - Hands-On: Probabilistic Programming Examples\nSession 5 — Optimization: The Strategy of Search - Convex vs Non-Convex Optimization - Optimization Algorithms & Convergence - Hyperparameter Tuning Strategies - Constraints & Regularization - Hands-On: Optimization Landscape Exploration\nSession 6 — Pillars in Practice: Advanced Architectures - Neural Networks Through Mathematical Lenses - Transformers & Attention Mechanisms - Generative Models & Statistical Physics Connections - Ethical AI & Mathematical Fairness - Case Study: End-to-End Mathematical Deconstruction\nSession 7 — Synthesis & Teaching Excellence - Micro-Teaching Lab - Curriculum Integration Strategies - Assessment Design for Conceptual Understanding - Community Building & Resource Sharing - Final Commitments & Action Plans\n\n\n\nCore Teaching Methodologies\n\nThe 5-Step Framework: Concept → Intuition → Mathematics → Code → Interpretation\nMathematical Storytelling & Conceptual Analogies\nVisual Demonstrations & Geometric Intuition\nHistorical Context & Philosophical Connections\nPedagogical Patterns for Abstract Concepts\n\nTools & Technologies\n\nPython with NumPy, SciPy, Matplotlib\nJupyter Notebook for Interactive Demonstrations\nVisualization Libraries for Mathematical Concepts\nLaTeX for Mathematical Notation\nCollaborative Platforms for Resource Sharing\n\nExpected Trainer Deliverables\n\nComprehensive pillar mapping of assigned curriculum\nIntuitive explanations for complex mathematical concepts\nVisual demonstrations and teaching analogies\nAssessment strategies for conceptual understanding\nLesson plans integrating the four pillars framework\n\nProgram Outcomes\n\nDeliver AI/ML content with strong mathematical foundations\nExplain complex concepts with clarity and intuition\nConnect theoretical mathematics to practical applications\nFoster deep conceptual understanding in students\nCreate a community of mathematically-grounded AI educators\n\n\n\n\n\n\nWe would greatly appreciate it if you could take a few moments to complete our feedback form.\nPlease click the link below or scan the QR code to access the feedback form:\nhttps://forms.gle/xxitNpH2Xr4ogACf6",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "feedback.html",
    "href": "feedback.html",
    "title": "Feedback Analysis",
    "section": "",
    "text": "We value your feedback! Please take a moment to provide your insights on the workshop. Your responses will help us improve and tailor future sessions to better meet your needs.\n\n\nPlease rate the following statements on a scale of 1 to 5, where 1 is “Strongly Disagree” and 5 is “Strongly Agree”:\n\nThe workshop objectives were clearly defined and effectively communicated.\n\n1\n2\n3\n4\n5\n\nThe workshop enhanced my understanding of the practical applications of Graph Theory in engineering and computing?\n\n1\n2\n3\n4\n5\n\nThe hands-on sessions and demonstrations using Python and NetworkX were helpful for practical learning.\n\n1\n2\n3\n4\n5\n\nThe content was well-structured, and the pace of delivery was appropriate.\n\n1\n2\n3\n4\n5\n\nI feel more confident in applying graph-based reasoning to real-world problem-solving after attending this workshop.\n\n1\n2\n3\n4\n5\n\n\nThank you for your feedback!\n\nYou can access the feedback form through the link below or scan the QR code to provide your responses:\nhttps://forms.gle/AD6vuxrPoohj7iz16"
  },
  {
    "objectID": "feedback.html#workshop-feedback-questionnaire",
    "href": "feedback.html#workshop-feedback-questionnaire",
    "title": "Feedback Analysis",
    "section": "",
    "text": "We value your feedback! Please take a moment to provide your insights on the workshop. Your responses will help us improve and tailor future sessions to better meet your needs.\n\n\nPlease rate the following statements on a scale of 1 to 5, where 1 is “Strongly Disagree” and 5 is “Strongly Agree”:\n\nThe workshop objectives were clearly defined and effectively communicated.\n\n1\n2\n3\n4\n5\n\nThe workshop enhanced my understanding of the practical applications of Graph Theory in engineering and computing?\n\n1\n2\n3\n4\n5\n\nThe hands-on sessions and demonstrations using Python and NetworkX were helpful for practical learning.\n\n1\n2\n3\n4\n5\n\nThe content was well-structured, and the pace of delivery was appropriate.\n\n1\n2\n3\n4\n5\n\nI feel more confident in applying graph-based reasoning to real-world problem-solving after attending this workshop.\n\n1\n2\n3\n4\n5\n\n\nThank you for your feedback!\n\nYou can access the feedback form through the link below or scan the QR code to provide your responses:\nhttps://forms.gle/AD6vuxrPoohj7iz16"
  },
  {
    "objectID": "feedback.html#response-analysis",
    "href": "feedback.html#response-analysis",
    "title": "Feedback Analysis",
    "section": "Response Analysis",
    "text": "Response Analysis\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the responses data\n# Replace 'feedback_responses.csv' with the path to your feedback form responses\nresponses_df = pd.read_csv('feedback_responses.csv')\n\n# Filter numerical columns\nnumerical_columns = responses_df.select_dtypes(include=['number']).columns\nnumerical_df = responses_df[numerical_columns]\n\n#short_names = {col: f'Q{i+1}' for i, col in enumerate(numerical_df.columns)}\n# Define short names dynamically, excluding the last column\nshort_names = {col: f'Q{i+1}' for i, col in enumerate(numerical_df.columns)}\n\n# Add the last column with its original name\n#short_names[numerical_df.columns[-1]] = f'Q{len(short_names)+1}'\n\n# Ensure that the short_headings match the exact column names in your DataFrame\nnumerical_df.rename(columns=short_names, inplace=True)\n\n# Display the first few rows of the numerical dataframe to understand its structure\n#print(numerical_df.head())\n\n# Statistical summary of the numerical feedback responses\n#summary_stats = numerical_df.describe()\n\n# Calculate count, mean, and standard deviation for each numerical question\nquestion_stats = numerical_df.agg(['count', 'mean', 'std'])\n\n# Display the summary statistics and question stats\n#print(\"\\nStatistical Summary:\")\n#print(summary_stats)\n\nprint(\"\\nQuestion Statistics:\")\nprint(question_stats)\n\n# Plot distribution of responses for each numerical question\n# Plot pie charts for each numerical question\nfor question in numerical_df.columns:\n    plt.figure(figsize=(6, 5))\n    \n    # Count responses for each score\n    counts = numerical_df[question].value_counts().sort_index()\n    \n    # Plot pie chart\n    plt.pie(counts, labels=counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired(range(len(counts))))\n    \n    plt.title(f'Pie Chart of Responses for {question}')\n    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    \n    # Show the plot\n    plt.show()\n\n# Plot heatmap of mean responses for numerical questions\nmean_responses = numerical_df.mean()\nplt.figure(figsize=(6, 5))\nsns.heatmap(mean_responses.values.reshape(-1, 1), annot=True, fmt='.2f', yticklabels=numerical_df.columns, cmap='coolwarm', cbar=True)\nplt.title('Mean Responses Heatmap')\nplt.xlabel('Mean Score')\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site contains information about a ToT organised by ASAP Kerala. This is part of the add-on program organized by the curriculum division, ASAP- Kerala.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "about.html#organizors",
    "href": "about.html#organizors",
    "title": "About",
    "section": "Organizors",
    "text": "Organizors\nCurriculum Division, Additional Skill Acquisition Programme Kerala, Government of Kerala",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "about.html#workshop-faculty",
    "href": "about.html#workshop-faculty",
    "title": "About",
    "section": "Workshop Faculty",
    "text": "Workshop Faculty\n\n\n\n\n\n\nSiju K S\n\n\n\n\n\n\n\nDr.Gilesh M.P\n\n\n\n\n\n\n\nDr. Merina Aruja",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "about.html#sidebar-image",
    "href": "about.html#sidebar-image",
    "title": "About",
    "section": "Sidebar image",
    "text": "Sidebar image\nThe sidebar image was generated by Microsoft Copilot with a prompt, “Transforming Classical Libraries to Digital Libraries with Open source.”",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "slides.html#agenda",
    "href": "slides.html#agenda",
    "title": "Hands-on Session on Quarto",
    "section": "Agenda",
    "text": "Agenda\n\nIntroduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming"
  },
  {
    "objectID": "slides.html#get-started",
    "href": "slides.html#get-started",
    "title": "Hands-on Session on Quarto",
    "section": "Get Started",
    "text": "Get Started\nDownloading Quarto\n\nDownload Quarto\n\nLinux\nMacOS\nWindows\n\nChoose your platform\n\nVS Code\nJupyter Notebook/Lab\nNeoVim\nRStudio"
  },
  {
    "objectID": "slides.html#listings",
    "href": "slides.html#listings",
    "title": "Hands-on Session on Quarto",
    "section": "Listings",
    "text": "Listings\n\nListings enable you to automatically generate the contents of a page (or region of a page) from a list of Quarto documents or other custom data\nUseful to create blogs, newletters\nLink to the documentation: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "slides.html#templates-and-customization",
    "href": "slides.html#templates-and-customization",
    "title": "Hands-on Session on Quarto",
    "section": "Templates and Customization",
    "text": "Templates and Customization\n\nSearch bar\n\nhttps://quarto.org/docs/websites/website-search.html\n\nThemes\n\nhttps://quarto.org/docs/output-formats/html-themes.html\n\nTools\n\nhttps://quarto.org/docs/websites/website-tools.html"
  },
  {
    "objectID": "slides.html#programing",
    "href": "slides.html#programing",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nQuarto also provides the option to embed some code on your website\nQuarto supports Python, R, Julia and Observable Javascript\nYou can create a code block delimiting using ```\nExample of code running: https://tailor-uob.github.io/training-material/cha_odm/odm.html"
  },
  {
    "objectID": "slides.html#programing-1",
    "href": "slides.html#programing-1",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nAlso Quarto allows the creation of short codes.\nShortcodes are special markdown directives that generate various types of content. Quarto shortcodes are similar in form and function to Hugo shortcodes and WordPress shortcodes.\nDocumentation: https://quarto.org/docs/extensions/shortcodes.html"
  },
  {
    "objectID": "hands_on.html",
    "href": "hands_on.html",
    "title": "Hands-on Session on Quarto",
    "section": "",
    "text": "Introduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming"
  },
  {
    "objectID": "hands_on.html#agenda",
    "href": "hands_on.html#agenda",
    "title": "Hands-on Session on Quarto",
    "section": "",
    "text": "Introduction to Quarto\nGet Started\nHands-on\n\nListings\nTemplates and Customization\nPrograming"
  },
  {
    "objectID": "hands_on.html#get-started",
    "href": "hands_on.html#get-started",
    "title": "Hands-on Session on Quarto",
    "section": "Get Started",
    "text": "Get Started\n\nDownloading Quarto\n\nDownload Quarto\n\nLinux\nMacOS\nWindows\n\nChoose your platform\n\nVS Code\nJupyter Notebook/Lab\nNeoVim\nRStudio\n\n\n\n\n\n\nDownload Quarto\n\nDownload Quarto: https://quarto.org/docs/get-started/\nWorkshop website: https://sijuswamy.github.io/CME-workshop-1/\n\n\nSetup\n\nTerminal + Text Editor (VS Code)\nVisual Studio Code has options to use quarto in a better user interface\n\n\n\nRun the command ‘quarto create project website ’\nAlternatively, ctrl+shift+p and create the quarto project\nAutomatically Quarto will create the following directory structure:\n\n_quarto.yml\nindex.qmd\nabout.qmd\nstyles.css\n\n\n\n\nStructure\n\n.yml files\n\nYAML is a human-readable data serialization language\nYAML is an official strict superset of JSON despite looking very different from JSON.\nTo create a YAML file, use either the .yaml or .yml file extension.\n\n.qmd files\n\nWork as markdown files, but they have a configuration section in yml on the top of the file\n\n\n\n\n_quarto.yml\n\nDefines the basic structure of the website.\nAll configurations are done using yml\nSome of the configurations:\n\nNavigation bar\nSide bar\nContents"
  },
  {
    "objectID": "hands_on.html#listings",
    "href": "hands_on.html#listings",
    "title": "Hands-on Session on Quarto",
    "section": "Listings",
    "text": "Listings\n\nListings enable you to automatically generate the contents of a page (or region of a page) from a list of Quarto documents or other custom data\nUseful to create blogs, newletters\nLink to the documentation: https://quarto.org/docs/websites/website-listings.html"
  },
  {
    "objectID": "hands_on.html#templates-and-customization",
    "href": "hands_on.html#templates-and-customization",
    "title": "Hands-on Session on Quarto",
    "section": "Templates and Customization",
    "text": "Templates and Customization\n\nSearch bar\n\nhttps://quarto.org/docs/websites/website-search.html\n\nThemes\n\nhttps://quarto.org/docs/output-formats/html-themes.html\n\nTools\n\nhttps://quarto.org/docs/websites/website-tools.html"
  },
  {
    "objectID": "hands_on.html#programing",
    "href": "hands_on.html#programing",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nQuarto also provides the option to embed some code on your website\nQuarto supports Python, R, Julia and Observable Javascript\nYou can create a code block delimiting using ```\nExample of code running: https://tailor-uob.github.io/training-material/cha_odm/odm.html"
  },
  {
    "objectID": "hands_on.html#programing-1",
    "href": "hands_on.html#programing-1",
    "title": "Hands-on Session on Quarto",
    "section": "Programing",
    "text": "Programing\n\nAlso Quarto allows the creation of short codes.\nShortcodes are special markdown directives that generate various types of content. Quarto shortcodes are similar in form and function to Hugo shortcodes and WordPress shortcodes.\nDocumentation: https://quarto.org/docs/extensions/shortcodes.html"
  },
  {
    "objectID": "introduction/index.html#introduction",
    "href": "introduction/index.html#introduction",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Introduction",
    "text": "Introduction\nGraph Theory forms a foundational pillar across modern Computer Science and Engineering. From the historic Königsberg Bridge Problem raised by Leonhard Euler in 1736 to today’s cutting-edge applications in Artificial Intelligence, Machine Learning, Cyber-security, VLSI, IoT, Intelligent Transportation, and Social Network Analytics, graph structures provide a natural mathematical way to represent relationships, connectivity, and optimization.\nThis session is designed to bridge the gap between Discrete Mathematics and real-world engineering applications, demonstrating how mathematical graph concepts translate into practical computational tools."
  },
  {
    "objectID": "introduction/index.html#learning-objectives",
    "href": "introduction/index.html#learning-objectives",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of this session, participants will be able to:\n\nUnderstand the evolution of graph theory from classical problems to modern computational applications.\nModel real scenarios as graphs and analyze connectivity, traversal, and optimization.\nExplore graph algorithms implemented through simulation and programming.\nRecognize the role of graph theory in AI, ML, and data-driven intelligent systems.\nDevelop intuition for advanced concepts such as graph coloring, network flows, and spectral graph theory.\n\nThis module introduces the historical evolution of graph theory, beginning with the classical foundations laid by Leonhard Euler (1736), and follows its transformation into a cornerstone of modern engineering and computational technologies."
  },
  {
    "objectID": "introduction/index.html#historical-starting-point-the-königsberg-bridges-problem",
    "href": "introduction/index.html#historical-starting-point-the-königsberg-bridges-problem",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Historical Starting Point: The Königsberg Bridges Problem",
    "text": "Historical Starting Point: The Königsberg Bridges Problem\nThe earliest known problem leading to graph theory originated from the city of Königsberg (now Kaliningrad, Russia), divided by the river Pregel with seven bridges linking different land masses. Citizens wondered:\n\nIs there a walk through the city that crosses each bridge exactly once?\n\nEuler abstracted the problem by representing landmasses as vertices and bridges as edges, forming the world’s first mathematical graph.\n\nEuler’s Key Result\nA connected graph admits an Eulerian circuit if and only if every vertex has even degree.\nGeneral case: A graph contains an Eulerian path if exactly 0 or 2 vertices have odd degree.\nEuler’s solution marks the birth of topological graph theory and the first example of modelling reality with abstraction."
  },
  {
    "objectID": "introduction/index.html#development-through-the-19th-20th-centuries",
    "href": "introduction/index.html#development-through-the-19th-20th-centuries",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Development Through the 19th & 20th Centuries",
    "text": "Development Through the 19th & 20th Centuries\n\n\n\n\n\n\n\n\nYear\nContributor\nContribution\n\n\n\n\n1847\nGustav Kirchhoff\nElectrical network theory & spanning trees in circuits\n\n\n1857\nArthur Cayley\nCounting labelled trees, chemical molecular structures\n\n\n1913\nGeorge D. Birkhoff\nChromatic polynomial & map colouring\n\n\n1936\nKuratowski\nCharacterization of planar graphs\n\n\n1959\nEdsger Dijkstra\nShortest path algorithm\n\n\n1959\nErdős & Rényi\nRandom graph theory\n\n\n1973\nHopcroft–Karp\nEfficient bipartite matching\n\n\n1998\nBrin & Page\nPageRank: web search based on eigenvector centrality\n\n\n2017–present\nKipf & Welling\nGraph Neural Networks (GNNs)"
  },
  {
    "objectID": "introduction/index.html#evolution-from-mathematical-theory-to-engineering",
    "href": "introduction/index.html#evolution-from-mathematical-theory-to-engineering",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Evolution from Mathematical Theory to Engineering",
    "text": "Evolution from Mathematical Theory to Engineering\nGraph theory transitioned from pure mathematics to an essential tool for computation and real-world problem solving. Today, nearly every complex engineering system is modelled using graph structures.\n\nWhy Engineers Need Graph Theory\nGraphs enable engineers to analyze, optimize, and predict system behavior.\n\n\n\n\n\n\n\n\n\nDomain\nEngineering Problem\nGraph Concept\nAlgorithmic Tool\n\n\n\n\nComputer Networks\nPacket routing, topology design\nWeighted graphs\nDijkstra / Bellman–Ford\n\n\nArtificial Intelligence\nKnowledge graphs, reasoning, GNNs\nDirected labelled graphs\nPageRank, embeddings\n\n\nVLSI & CAD\nCircuit layout & track minimisation\nGraph colouring\nGreedy, planar colouring\n\n\nTransport & Robotics\nNavigation, path planning\nShortest paths\nA*, BFS\n\n\nCybersecurity\nAttack spread modelling\nAttack graphs\nCentrality\n\n\nSocial Networks\nInfluence, community detection\nSmall-world graphs\nModularity, clustering\n\n\nBig Data\nLarge-scale graph analytics\nDistributed graphs\nMapReduce / Spark GraphX\n\n\nHealthcare\nDisease spread & medical networks\nDynamic graphs\nSIR modelling"
  },
  {
    "objectID": "introduction/index.html#key-ideas-that-form-the-foundations-of-graph-algorithms",
    "href": "introduction/index.html#key-ideas-that-form-the-foundations-of-graph-algorithms",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Key Ideas That Form the Foundations of Graph Algorithms",
    "text": "Key Ideas That Form the Foundations of Graph Algorithms\n\nTraversal: BFS, DFS for exploring nodes and connectivity\nOptimisation: Minimal spanning trees, flows, cut problems\nRanking & Influence: Centrality, PageRank\nScheduling & Resources: Graph colouring\nStructure Discovery: Communities & clustering\nLearning: GNNs for relational intelligence\n\nMathematically, a graph is defined as:\n\\[ G = (V, E) \\] where\n- \\(V\\) is a set of vertices,\n- \\(E\\) is a set of edges connecting pairs of vertices."
  },
  {
    "objectID": "introduction/index.html#demonstration-representing-reality-through-graph-abstraction",
    "href": "introduction/index.html#demonstration-representing-reality-through-graph-abstraction",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Demonstration: Representing Reality through Graph Abstraction",
    "text": "Demonstration: Representing Reality through Graph Abstraction\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nG = nx.Graph()\nG.add_edges_from([(\"Island 1\",\"Island 2\"),(\"Island 1\",\"Island 3\"),\n                  (\"Island 2\",\"Island 3\"),(\"Island 3\",\"Island 4\"),\n                  (\"Island 4\",\"Island 1\")])\nnx.draw(G, with_labels=True)\nplt.show()"
  },
  {
    "objectID": "introduction/index.html#conclusion",
    "href": "introduction/index.html#conclusion",
    "title": "AI Mission in the Age of AI- Training of Trainers Program",
    "section": "Conclusion",
    "text": "Conclusion\nGraph Theory evolved from Euler’s solution to a historic puzzle into a powerful computational language for solving real-world problems. It now forms the mathematical backbone of modern engineering systems — from internet routing and GPS navigation to web search engines, cybersecurity, and intelligent AI models such as Graph Neural Networks.\n\nGraph Theory connects mathematical reasoning with engineering innovation, transforming abstract structures into practical solutions."
  },
  {
    "objectID": "Session2/slides-ML.html#the-unseen-architecture-of-intelligence",
    "href": "Session2/slides-ML.html#the-unseen-architecture-of-intelligence",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "The Unseen Architecture of Intelligence",
    "text": "The Unseen Architecture of Intelligence\n\n\n\n\n\n\nKey points\n\n\nLinear algebra forms:\n\nBlue print\nBuilding materials\nLanguage to communicate"
  },
  {
    "objectID": "Session2/slides-ML.html#the-vector-the-fundamental-unit-of-intelligence",
    "href": "Session2/slides-ML.html#the-vector-the-fundamental-unit-of-intelligence",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "The Vector: The Fundamental Unit of Intelligence",
    "text": "The Vector: The Fundamental Unit of Intelligence\n\nData as a point in multi-dimensional space\nCan be translated as a sample in data science perspective\nDot product is a measure of similarity\n\ncosine similarity in recommendation system\nattension mechanism in transformers\nfeature correlation in feature analysis"
  },
  {
    "objectID": "Session2/slides-ML.html#matrices-the-engines-of-transformation",
    "href": "Session2/slides-ML.html#matrices-the-engines-of-transformation",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Matrices: The Engines of Transformation",
    "text": "Matrices: The Engines of Transformation\n\nData as organized form of row information\nMatrix multiplication as affine transformations\n\ncovariance matrix reveals the structure of dataset"
  },
  {
    "objectID": "Session2/slides-ML.html#eigen-decomposition-finding-natural-directions",
    "href": "Session2/slides-ML.html#eigen-decomposition-finding-natural-directions",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Eigen Decomposition: Finding Natural Directions",
    "text": "Eigen Decomposition: Finding Natural Directions\n\nCharacteristic equation: invarient vectors under linear transformation\nEigen vectors: directions in which higher variance observed\nPCA- dimensionality reduction"
  },
  {
    "objectID": "Session2/slides-ML.html#singular-value-decomposition-the-ultimate-factorization",
    "href": "Session2/slides-ML.html#singular-value-decomposition-the-ultimate-factorization",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Singular Value Decomposition: The Ultimate Factorization",
    "text": "Singular Value Decomposition: The Ultimate Factorization\nAny matrix can be decomposed as:\n\\[\n\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\n\n\n\n\n\n\nSVD in ML\n\n\n\nUsed in recommedation system\nUsed in Latent Sematic Analysis\nUsed in missing value imputation\nUsed in dimensionality reduction"
  },
  {
    "objectID": "Session2/slides-ML.html#teaching-linear-algebra-with-intuition",
    "href": "Session2/slides-ML.html#teaching-linear-algebra-with-intuition",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Teaching Linear Algebra with Intuition",
    "text": "Teaching Linear Algebra with Intuition\n\nstart from excel sheets to create the intution of sample space and feature space\nblind scuptor anology for eigen vectors\ndot product as a linear combination\nmatrix multiplication as a mixing problem\nPCA as formulating a coordinate system for the dataset"
  },
  {
    "objectID": "Session2/slides-ML.html#linear-algebra-in-neural-networks",
    "href": "Session2/slides-ML.html#linear-algebra-in-neural-networks",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Linear Algebra in Neural Networks",
    "text": "Linear Algebra in Neural Networks\n\nkey tool for prediction by feature mixing\nlearn patterns\nenable heirarchical learning\nconvolution as sparse multiplication\nattention mechanism as weighted combination"
  },
  {
    "objectID": "Session2/slides-ML.html#common-student-challenges-and-solutions",
    "href": "Session2/slides-ML.html#common-student-challenges-and-solutions",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\nWhy do we need matrices instead of just doing things one point at a time?\nEigenvalues and eigenvectors seem too abstract!\nWhen would I use SVD in real applications?"
  },
  {
    "objectID": "Session2/slides-ML.html#mission-bilding-linear-algebra-intuition",
    "href": "Session2/slides-ML.html#mission-bilding-linear-algebra-intuition",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Mission: Bilding Linear Algebra Intuition",
    "text": "Mission: Bilding Linear Algebra Intuition\n\nCreate a geometric interpreter using LA\nCreate an anology architect for main components of LA\nCreate a mindmap for LA to AI"
  },
  {
    "objectID": "Session2/slides-ML.html#references",
    "href": "Session2/slides-ML.html#references",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Session2/index.html#the-unseen-architecture-of-intelligence",
    "href": "Session2/index.html#the-unseen-architecture-of-intelligence",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "The Unseen Architecture of Intelligence",
    "text": "The Unseen Architecture of Intelligence\n\n\n\n\n\n\nKey points\n\n\n\nLinear algebra forms:\n\nBlue print\nBuilding materials\nLanguage to communicate\n\n\n\nWelcome to our deep exploration of Linear Algebra, the first and most fundamental pillar of artificial intelligence. If we consider AI as a new form of architecture, then linear algebra provides both the blueprint and the building materials. Every intelligent system, from the simplest classifier to the most complex large language model, is fundamentally built upon the principles we will explore today.\nLinear algebra gives us the mathematical language to describe, manipulate, and transform data. It provides the structural framework that allows us to move from individual data points to meaningful patterns, from isolated measurements to comprehensive understanding. In this session, we will uncover how vectors, matrices, and linear transformations form the invisible scaffolding that supports all of modern AI (Deisenroth, Faisal, and Ong (2020), (strang2022introduction?))."
  },
  {
    "objectID": "Session2/index.html#the-vector-the-fundamental-unit-of-intelligence",
    "href": "Session2/index.html#the-vector-the-fundamental-unit-of-intelligence",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "The Vector: The Fundamental Unit of Intelligence",
    "text": "The Vector: The Fundamental Unit of Intelligence\n\nData as a point in multi-dimensional space\nCan be translated as a sample in data science perspective\nDot product is a measure of similarity\n\ncosine similarity in recommendation system\nattension mechanism in transformers\nfeature correlation in feature analysis\n\n\n\nBest introduction of vectors from data science\n\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\n\n# Create a Pandas DataFrame for better readability\n# The 'data' attribute contains the features, and 'feature_names' are the column labels\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\n# Add the target variable (species) to the DataFrame\n# 'target' contains numerical labels (0, 1, 2), and 'target_names' maps them to species names\ndf['species'] = iris.target_names[iris.target]\n\n# Display the first few rows of the DataFrame\nprint(df.head())\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n  species  \n0  setosa  \n1  setosa  \n2  setosa  \n3  setosa  \n4  setosa  \n\n\n\n\nData as Points in Space\nConsider our familiar Iris dataset. Each flower, with its four measurements, becomes a point in a four-dimensional space:\niris_flower = [5.1, 3.5, 1.4, 0.2] # A vector in R^4\nThis representation is profound. We can now measure similarity between flowers using Euclidean distance:\n\\[\nd(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^4 (x_i - y_i)^2}\n\\]\nWe can compute centroids for each species:\n\\[\n\\mathbf{\\mu}_{\\text{setosa}} = \\frac{1}{50} \\sum_{i=1}^{50} \\mathbf{x}_i\n\\]\nAnd we can measure how spread out each species is around its center. This geometric perspective transforms abstract statistics into spatial intuition.\n\n\nThe Dot Product: Measuring Alignment and Similarity\nThe dot product between two vectors reveals their alignment:\n\\[\n\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i = \\|\\mathbf{a}\\|\\|\\mathbf{b}\\|\\cos\\theta\n\\]\nThis simple operation underpins:\n\nCosine similarity in recommendation systems\nAttention mechanisms in transformers\nFeature correlations in data analysis\n\nWhen two vectors point in similar directions, their dot product is large. When they’re perpendicular, it’s zero. This geometric insight helps students understand why certain mathematical operations work the way they do."
  },
  {
    "objectID": "Session2/index.html#matrices-the-engines-of-transformation",
    "href": "Session2/index.html#matrices-the-engines-of-transformation",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Matrices: The Engines of Transformation",
    "text": "Matrices: The Engines of Transformation\n\nData as organized form of row information\nMatrix multiplication as affine transformations\n\ncovariance matrix reveals the structure of dataset\n\n\n\nData as Organized Information\nOur entire Iris dataset forms a matrix:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n5.1 & 3.5 & 1.4 & 0.2 \\\\\n4.9 & 3.0 & 1.4 & 0.2 \\\\\n4.7 & 3.2 & 1.3 & 0.2 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\end{bmatrix}\n\\]\nThis matrix representation enables powerful operations. We can center our data by subtracting the mean vector, scale features to comparable ranges, and compute the covariance matrix that reveals how features vary together.\n\n\nMatrix Multiplication as Transformation\nWhen we multiply a data matrix by a weight matrix, we’re performing a linear transformation:\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{W}\n\\]\nEach column of \\(\\mathbf{W}\\) defines a new “direction” or “feature” in our transformed space. In neural networks, these transformations are followed by non-linear activations, but the linear component provides the fundamental structure.\n\n\nThe Covariance Matrix: Revealing Data Structure\nThe covariance matrix:\n\\[\n\\mathbf{\\Sigma} = \\frac{1}{n}\\mathbf{X}^T\\mathbf{X}\n\\]\nencodes how our features co-vary. Its eigenvectors point in directions of maximum variance, and its eigenvalues tell us how much variance lies in each direction. This is the mathematical foundation of Principal Component Analysis (PCA)."
  },
  {
    "objectID": "Session2/index.html#eigen-decomposition-finding-natural-directions",
    "href": "Session2/index.html#eigen-decomposition-finding-natural-directions",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Eigen Decomposition: Finding Natural Directions",
    "text": "Eigen Decomposition: Finding Natural Directions\n\nCharacteristic equation: invarient vectors under linear transformation\nEigen vectors: directions in which higher variance observed\nPCA- dimensionality reduction\n\n\nThe Eigenvalue Equation\nThe fundamental equation:\n\\[\n\\mathbf{A}\\mathbf{v} = \\lambda\\mathbf{v}\n\\]\ntells us that certain vectors (eigenvectors) only get stretched, not rotated, when transformed by a matrix. The stretching factor is the eigenvalue.\nFor our Iris dataset, the eigenvectors of the covariance matrix point in the “natural directions” of the data—the directions where the flowers show the most variation. The corresponding eigenvalues tell us how much variation exists in each natural direction.\n\n\nPrincipal Component Analysis: Dimensionality Reduction in Action\nPCA finds these natural directions and projects our data onto them:\n\nCenter the data: \\(\\mathbf{X}_{\\text{centered}} = \\mathbf{X} - \\mathbf{\\mu}\\)\nCompute covariance: \\(\\mathbf{\\Sigma} = \\frac{1}{n}\\mathbf{X}_{\\text{centered}}^T\\mathbf{X}_{\\text{centered}}\\)\nFind eigenvectors: \\(\\mathbf{\\Sigma}\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i\\)\nProject data: \\(\\mathbf{Z} = \\mathbf{X}_{\\text{centered}}\\mathbf{V}\\)\n\nwhere \\(\\mathbf{V}\\) contains the top-k eigenvectors. This transformation often reveals that our four-dimensional Iris data actually lives in a lower-dimensional subspace where the species separate naturally."
  },
  {
    "objectID": "Session2/index.html#singular-value-decomposition-the-ultimate-factorization",
    "href": "Session2/index.html#singular-value-decomposition-the-ultimate-factorization",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Singular Value Decomposition: The Ultimate Factorization",
    "text": "Singular Value Decomposition: The Ultimate Factorization\nAny matrix can be decomposed as:\n\\[\n\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\n\n\n\n\n\n\nSVD in ML\n\n\n\n\nUsed in recommedation system\nUsed in Latent Sematic Analysis\nUsed in missing value imputation\nUsed in dimensionality reduction\n\n\n\n\nThe SVD Theorem\nAny matrix can be decomposed as:\n\\[\n\\mathbf{X} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T\n\\]\nwhere: - \\(\\mathbf{U}\\) contains the left singular vectors (patterns in rows/observations) - \\(\\mathbf{\\\\Sigma}\\) is diagonal with singular values (importance of each pattern) - \\(\\mathbf{V}\\) contains the right singular vectors (patterns in columns/features)\nFor our Iris data, SVD reveals that we can reconstruct the entire dataset using only a few important patterns. The singular values tell us how much each pattern contributes to the overall structure.\n\n\nSVD in Machine Learning\nSVD appears throughout machine learning:\n\nRecommendation systems (collaborative filtering)\nWord embeddings (Latent Semantic Analysis)\nMatrix completion (filling in missing values)\nDimensionality reduction (PCA is a special case of SVD)"
  },
  {
    "objectID": "Session2/index.html#teaching-linear-algebra-with-intuition",
    "href": "Session2/index.html#teaching-linear-algebra-with-intuition",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Teaching Linear Algebra with Intuition",
    "text": "Teaching Linear Algebra with Intuition\n\nstart from excel sheets to create the intution of sample space and feature space\nblind scuptor anology for eigen vectors\ndot product as a linear combination\nmatrix multiplication as a mixing problem\nPCA as formulating a coordinate system for the dataset\n\n\nThe “Blind Sculptor” Analogy for Eigenvectors\nImagine a sculptor working blindfolded. She can only feel the shape by poking it and seeing how it deforms. Eigenvectors are like the directions where poking just makes the shape uniformly larger or smaller, without changing its essential form. Eigenvalues tell her how much each poke stretches the shape.\n\n\nMatrix Multiplication as “Recipe Combination”\nThink of a matrix as a recipe book. Each column is a recipe for creating a new feature. Matrix multiplication is like applying all these recipes to your ingredients (data) simultaneously to create a transformed meal (new representation).\n\n\nThe “Data Compass” for PCA\nPrincipal components are like a compass for your data. The first principal component points toward the direction of greatest variation—the “north” of your dataset. The second points toward the next most important direction, and so on. By following this compass, we can navigate high-dimensional spaces effectively."
  },
  {
    "objectID": "Session2/index.html#linear-algebra-in-neural-networks",
    "href": "Session2/index.html#linear-algebra-in-neural-networks",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Linear Algebra in Neural Networks",
    "text": "Linear Algebra in Neural Networks\n\nkey tool for prediction by feature mixing\nlearn patterns\nenable heirarchical learning\nconvolution as sparse multiplication\nattention mechanism as weighted combination\n\n\nThe Fundamental Operation\nEvery layer in a neural network performs:\n\\[\n\\mathbf{Z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b}\n\\]\nfollowed by a non-linear activation \\(\\sigma(\\mathbf{Z})\\). This linear transformation:\n\nMixes features to create new representations\nLearns patterns through the weight matrix\nEnables hierarchical learning through multiple layers\n\n\n\nConvolution as Matrix Multiplication\nConvolutional layers in neural networks can be viewed as matrix multiplications with sparse, structured weight matrices. This perspective helps students understand that CNNs are fundamentally linear algebra with additional constraints.\n\n\nAttention Mechanisms as Weighted Combinations\nThe attention mechanism in transformers computes:\n\\[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\n\\]\nThis is fundamentally linear algebra: we compute similarities (dot products), normalize them (softmax), and use them to form weighted combinations of values."
  },
  {
    "objectID": "Session2/index.html#common-student-challenges-and-solutions",
    "href": "Session2/index.html#common-student-challenges-and-solutions",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\nWhy do we need matrices instead of just doing things one point at a time?\nEigenvalues and eigenvectors seem too abstract!\nWhen would I use SVD in real applications?\n\n\nChallenge 1: “Why do we need matrices instead of just doing things one point at a time?”\nSolution: Use the “orchestra analogy.” Working with individual data points is like listening to each musician separately. Working with matrices is like hearing the entire orchestra—you understand how all the parts work together to create something greater than the sum of its parts.\n\n\nChallenge 2: “Eigenvalues and eigenvectors seem too abstract.”\nSolution: Connect them to concrete examples. In the Iris dataset, the first eigenvector might point in the “petal size” direction, and its eigenvalue tells us how much the flowers vary in petal size compared to other characteristics.\n\n\nChallenge 3: “When would I use SVD in real applications?”\nSolution: Show how Netflix might use SVD for recommendations, or how Google uses it for understanding word meanings in different contexts."
  },
  {
    "objectID": "Session2/index.html#mission-bilding-linear-algebra-intuition",
    "href": "Session2/index.html#mission-bilding-linear-algebra-intuition",
    "title": "Session 2 — Linear Algebra - The Language of Data and Transformations",
    "section": "Mission: Bilding Linear Algebra Intuition",
    "text": "Mission: Bilding Linear Algebra Intuition\n\nCreate a geometric interpreter using LA\nCreate an anology architect for main components of LA\nCreate a mindmap for LA to AI\n\n\nAssignment 1: The Geometric Interpreter\nChoose one algorithm from your teaching module that uses linear algebra extensively. Create a visual explanation showing:\n\nHow the data is represented as vectors/matrices\nWhat linear transformations occur\nHow eigenvalues/eigenvectors or SVD components appear\nWhat the geometric interpretation of the algorithm is\n\n\n\nAssignment 2: The Analogy Architect\nDevelop three teaching analogies for linear algebra concepts:\n\nOne for matrix multiplication\nOne for eigenvectors/eigenvalues\nOne for dimensionality reduction\n\nEach analogy should be accessible to students with minimal mathematical background yet mathematically precise enough to build correct intuition.\n\n\nAssignment 3: The Connection Map\nCreate a diagram showing how linear algebra concepts connect across different AI domains:\n\nNeural networks\nComputer vision\nNatural language processing\nRecommendation systems\nIdentify where the same mathematical concept (e.g., dot products, matrix factorization) appears in different contexts."
  },
  {
    "objectID": "Session4/slides-ML.html#the-foundation-of-intelligent-decision-making",
    "href": "Session4/slides-ML.html#the-foundation-of-intelligent-decision-making",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "The Foundation of Intelligent Decision-Making",
    "text": "The Foundation of Intelligent Decision-Making\n\nProbability theory makes you an intelligent descision maker\nHelp to moving beyond the observed limits\nMake the models reliable\nOne of the most scientific backbone of advanced AI"
  },
  {
    "objectID": "Session4/slides-ML.html#probability-theory-the-mathematics-of-uncertainty",
    "href": "Session4/slides-ML.html#probability-theory-the-mathematics-of-uncertainty",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Probability Theory: The Mathematics of Uncertainty",
    "text": "Probability Theory: The Mathematics of Uncertainty\n\nClassical probability as measure of chance based on frequency\nProbability axioms\nClassical definition as relative measure/ rate\nLimitations of classical probability\nRandom variable as bulding block for insightful inference\nTheoretical distributions and its importance\nMulti-variate random variables and different types of distributions"
  },
  {
    "objectID": "Session4/slides-ML.html#bayesian-inference-updating-beliefs-with-evidence",
    "href": "Session4/slides-ML.html#bayesian-inference-updating-beliefs-with-evidence",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Bayesian Inference: Updating Beliefs with Evidence",
    "text": "Bayesian Inference: Updating Beliefs with Evidence\n\nTransformation from frequentistic to measure of belief\nConditional probability\nBayes; theorem\nBayesian Classifier\nconjugate priors and its uses in probabilistic models"
  },
  {
    "objectID": "Session4/slides-ML.html#statistical-inference-drawing-conclusions-from-data",
    "href": "Session4/slides-ML.html#statistical-inference-drawing-conclusions-from-data",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Statistical Inference: Drawing Conclusions from Data",
    "text": "Statistical Inference: Drawing Conclusions from Data\n\nDescriptive to inferential models\nEstimation theory\n\npoint estimates\ninterval estimates\n\nHypothesis testing\nparametric and non-parametric tests\n\nsmall sample tests\nlarge sample tests"
  },
  {
    "objectID": "Session4/slides-ML.html#distributions-in-machine-learning",
    "href": "Session4/slides-ML.html#distributions-in-machine-learning",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Distributions in Machine Learning",
    "text": "Distributions in Machine Learning\n\nExponential family\nMixture models"
  },
  {
    "objectID": "Session4/slides-ML.html#uncertainty-quantification-in-deep-learning",
    "href": "Session4/slides-ML.html#uncertainty-quantification-in-deep-learning",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Uncertainty Quantification in Deep Learning",
    "text": "Uncertainty Quantification in Deep Learning\n\nBayesian neural networks\nMote Carlo dropouts"
  },
  {
    "objectID": "Session4/slides-ML.html#hands-on-exploration-quantifying-uncertainty",
    "href": "Session4/slides-ML.html#hands-on-exploration-quantifying-uncertainty",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Hands-On Exploration: Quantifying Uncertainty",
    "text": "Hands-On Exploration: Quantifying Uncertainty\n\nBayesian Linear Regression\nConfidance intervals prediction"
  },
  {
    "objectID": "Session4/index.html#the-foundation-of-intelligent-decision-making",
    "href": "Session4/index.html#the-foundation-of-intelligent-decision-making",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "The Foundation of Intelligent Decision-Making",
    "text": "The Foundation of Intelligent Decision-Making\n\nProbability theory makes you an intelligent descision maker\nHelp to moving beyond the observed limits\nMake the models reliable\nOne of the most scientific backbone of advanced AI\n\nWelcome to our exploration of probability and statistics, the third pillar of artificial intelligence that provides the mathematical framework for reasoning under uncertainty. While linear algebra gives us the language to describe data and calculus provides the mechanism for learning, probability and statistics give us the wisdom to understand the limits of our knowledge and make intelligent decisions in the face of incomplete information.\nIn a world filled with noisy measurements, imperfect models, and inherent randomness, probability theory allows us to quantify uncertainty, while statistics enables us to draw meaningful conclusions from limited data. This session will reveal how these ancient mathematical disciplines form the bedrock of modern AI systems that must operate reliably in an uncertain world."
  },
  {
    "objectID": "Session4/index.html#probability-theory-the-mathematics-of-uncertainty",
    "href": "Session4/index.html#probability-theory-the-mathematics-of-uncertainty",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Probability Theory: The Mathematics of Uncertainty",
    "text": "Probability Theory: The Mathematics of Uncertainty\n\nClassical probability as measure of chance based on frequency\nProbability axioms\nClassical definition as relative measure/ rate\nLimitations of classical probability\nRandom variable as bulding block for insightful inference\nTheoretical distributions and its importance\nMulti-variate random variables and different types of distributions\n\n\nFoundations of Probability\nProbability provides a formal language for quantifying uncertainty. The basic axioms:\n\\[\n\\begin{aligned}\nP(A) &\\geq 0 \\quad \\text{for any event } A \\\\\\\\\nP(\\Omega) &= 1 \\quad \\text{where } \\Omega \\text{ is the sample space} \\\\\\\\\nP\\left(\\bigcup_{i=1}^\\infty A_i\\right) &= \\sum_{i=1}^\\infty P(A_i) \\quad \\text{for disjoint events}\n\\end{aligned}\n\\]\nThese simple rules form the foundation for all probabilistic reasoning in AI.\n\n\nRandom Variables and Distributions\nA random variable \\(X\\) assigns numerical values to outcomes. For our Iris dataset, we can model each feature as a random variable:\n\nSepal length \\(S_L \\sim \\text{Some Distribution}\\)\nPetal width \\(P_W \\sim \\text{Some Distribution}\\)\n\nCommon distributions in machine learning include:\nGaussian Distribution: \\[\np(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\nBernoulli Distribution: \\[\np(x|p) = p^x(1-p)^{1-x} \\quad \\text{for } x \\in \\\\{0,1\\\\}\n\\]\nCategorical Distribution: \\[\np(x|\\mathbf{p}) = \\prod_{k=1}^K p_k^{\\mathbb{I}(x=k)}\n\\]\n\n\nJoint, Marginal, and Conditional Probabilities\nFor multiple random variables, we have:\n\nJoint Probability: \\(P(X=x, Y=y)\\) - probability of both events occurring\nMarginal Probability: \\(P(X=x) = \\sum_y P(X=x, Y=y)\\) - probability of one event regardless of others\nConditional Probability: \\(P(X=x|Y=y) = \\frac{P(X=x, Y=y)}{P(Y=y)}\\) - probability of one event given another"
  },
  {
    "objectID": "Session4/index.html#bayesian-inference-updating-beliefs-with-evidence",
    "href": "Session4/index.html#bayesian-inference-updating-beliefs-with-evidence",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Bayesian Inference: Updating Beliefs with Evidence",
    "text": "Bayesian Inference: Updating Beliefs with Evidence\n\nTransformation from frequentistic to measure of belief\nConditional probability\nBayes; theorem\nBayesian Classifier\nconjugate priors and its uses in probabilistic models\n\n\nBayes’ Theorem: The Foundation of Learning\nBayes’ theorem provides the mathematical framework for updating beliefs in light of new evidence:\n\\[\nP(\\theta|\\mathcal{D}) = \\frac{P(\\mathcal{D}|\\theta)P(\\theta)}{P(\\mathcal{D})}\n\\]\nWhere:\n\n\\(P(\\theta|\\mathcal{D})\\) is the posterior - our updated belief about parameters \\(\\theta\\) after seeing data \\(\\mathcal{D}\\)\n\\(P(\\mathcal{D}|\\theta)\\) is the likelihood - how probable the data is under different parameters\n\\(P(\\theta)\\) is the prior - our belief about parameters before seeing data\n\\(P(\\mathcal{D})\\) is the evidence - a normalizing constant\n\n\n\nBayesian Classification for Iris Data\nFor classifying Iris flowers, we can use Bayes’ theorem:\n\\[\nP(\\text{species}=k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\text{species}=k)P(\\text{species}=k)}{\\sum_{j=1}^3 p(\\mathbf{x}|\\text{species}=j)P(\\text{species}=j)}\n\\]\nIf we assume each class has a Gaussian distribution:\n\\[\np(\\mathbf{x}|\\text{species}=k) = \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n\\]\nThis gives us a principled probabilistic classifier that naturally handles uncertainty.\n\n\nConjugate Priors and Computational Efficiency\nSome prior-likelihood pairs have nice mathematical properties called conjugacy:\n\nGaussian-Gaussian: If likelihood is Gaussian and prior is Gaussian, posterior is Gaussian\nBeta-Bernoulli: If likelihood is Bernoulli and prior is Beta, posterior is Beta\nDirichlet-Multinomial: If likelihood is Multinomial and prior is Dirichlet, posterior is Dirichlet\n\nThese conjugacies enable efficient Bayesian updating and are widely used in practice."
  },
  {
    "objectID": "Session4/index.html#statistical-inference-drawing-conclusions-from-data",
    "href": "Session4/index.html#statistical-inference-drawing-conclusions-from-data",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Statistical Inference: Drawing Conclusions from Data",
    "text": "Statistical Inference: Drawing Conclusions from Data\n\nDescriptive to inferential models\nEstimation theory\n\npoint estimates\ninterval estimates\n\nHypothesis testing\nparametric and non-parametric tests\n\nsmall sample tests\nlarge sample tests\n\n\n\nMaximum Likelihood Estimation (MLE)\nMLE finds parameters that make the observed data most probable:\n\\[\n\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta P(\\mathcal{D}|\\theta)\n\\]\nFor Gaussian distributions, MLE gives us familiar results:\n\n\\(\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\n\\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\hat{\\mu})^2\\)\n\n\n\nMaximum A Posteriori (MAP) Estimation\nMAP estimation incorporates prior knowledge:\n\\[\n\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta P(\\theta|\\mathcal{D}) = \\arg\\max_\\theta P(\\mathcal{D}|\\theta)P(\\theta)\n\\]\nThis provides a Bayesian alternative to MLE that can prevent overfitting.\n\n\nHypothesis Testing and Confidence Intervals\nFrequentist statistics provides tools for making inferences:\n\nConfidence Intervals: An interval that would contain the true parameter 95% of the time in repeated sampling\np-values: The probability of observing data as extreme as what we saw, assuming the null hypothesis is true\n\nWhile these concepts have limitations, they remain important for model evaluation and comparison."
  },
  {
    "objectID": "Session4/index.html#distributions-in-machine-learning",
    "href": "Session4/index.html#distributions-in-machine-learning",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Distributions in Machine Learning",
    "text": "Distributions in Machine Learning\n\nExponential family\nMixture models\n\n\nExponential Family Distributions\nMany important distributions belong to the exponential family:\n\\[\np(x|\\eta) = h(x) \\exp(\\eta^T T(x) - A(\\eta))\n\\]\nWhere:\n\n\\(\\eta\\) are natural parameters\n\\(T(x)\\) are sufficient statistics\n\\(A(\\eta)\\) is the log-normalizer\n\nThis family includes Gaussian, Bernoulli, Poisson, Gamma, and many other distributions, enabling unified treatment and efficient computation.\n\n\nMixture Models and Latent Variables\nMixture models combine multiple distributions to model complex data:\n\\[\np(\\mathbf{x}) = \\sum_{k=1}^K \\pi_k p_k(\\mathbf{x})\n\\]\nWhere \\(\\pi_k\\) are mixing weights and \\(p_k\\) are component distributions. The EM algorithm provides an efficient way to learn mixture models."
  },
  {
    "objectID": "Session4/index.html#probabilistic-graphical-models",
    "href": "Session4/index.html#probabilistic-graphical-models",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Probabilistic Graphical Models",
    "text": "Probabilistic Graphical Models\n\nRepresenting Complex Dependencies\nGraphical models use graphs to represent probabilistic relationships:\n\nBayesian Networks: Directed acyclic graphs representing causal relationships\nMarkov Random Fields: Undirected graphs representing symmetric dependencies\n\nThese models enable efficient computation and intuitive representation of complex probabilistic systems.\n\n\nInference in Graphical Models\nKey inference tasks include:\n\nMarginal inference: \\(P(X_i) = \\sum_{\\mathbf{x}_{\\setminus i}} P(\\mathbf{x})\\)\nMaximum a posteriori (MAP): \\(\\arg\\max_{\\mathbf{x}} P(\\mathbf{x})\\)\nPosterior inference: \\(P(\\text{hidden}|\\text{observed})\\)\n\nAlgorithms like belief propagation, junction trees, and variational methods enable efficient inference."
  },
  {
    "objectID": "Session4/index.html#uncertainty-quantification-in-deep-learning",
    "href": "Session4/index.html#uncertainty-quantification-in-deep-learning",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Uncertainty Quantification in Deep Learning",
    "text": "Uncertainty Quantification in Deep Learning\n\nBayesian neural networks\nMote Carlo dropouts\n\n\nBayesian Neural Networks\nTraditional neural networks provide point estimates. Bayesian neural networks model uncertainty in weights:\n\\[\nP(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int P(\\mathbf{y}|\\mathbf{x}, \\mathbf{w})P(\\mathbf{w}|\\mathcal{D}) d\\mathbf{w}\n\\]\nThis provides uncertainty estimates for predictions, crucial for safety-critical applications.\n\n\nMonte Carlo Dropout\nA practical approximation for Bayesian neural networks uses dropout at test time:\n\\[\nP(\\mathbf{y}|\\mathbf{x}) \\approx \\frac{1}{T} \\sum_{t=1}^T P(\\mathbf{y}|\\mathbf{x}, \\mathbf{w}_t)\n\\]\nWhere \\(\\mathbf{w}_t\\) are different dropout masks, providing an ensemble of predictions.\n\n\nCalibration and Reliability\nWell-calibrated models should satisfy:\n\\[\nP(\\hat{Y} = Y | \\hat{P} = p) = p\n\\]\nWhere \\(\\hat{P}\\) is the predicted probability. Modern neural networks often need calibration techniques like temperature scaling."
  },
  {
    "objectID": "Session4/index.html#teaching-probability-with-intuition",
    "href": "Session4/index.html#teaching-probability-with-intuition",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Teaching Probability with Intuition",
    "text": "Teaching Probability with Intuition\n\nThe “Weather Forecast” Analogy for Bayesian Updates\nThink of Bayesian updating like a weather forecast. Your prior is yesterday’s forecast. New data is today’s weather observations. The posterior is tomorrow’s updated forecast. Each day, you combine old predictions with new evidence to improve future forecasts.\n\n\nThe “Medical Test” Example for Conditional Probability\nA medical test with 99% accuracy sounds reliable, but if the disease is rare (1 in 10,000), a positive test might only have ~1% chance of being correct. This counterintuitive result demonstrates why we need Bayesian reasoning and can’t just look at test accuracy alone.\n\n\nThe “Polling” Analogy for Confidence Intervals\nA political poll might show a candidate with 52% support ±3%. This doesn’t mean exactly 52% of voters support them, but that if we conducted the poll many times, 95% of the intervals would contain the true percentage. The uncertainty is about the method, not the candidate’s support."
  },
  {
    "objectID": "Session4/index.html#statistical-evaluation-of-models",
    "href": "Session4/index.html#statistical-evaluation-of-models",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Statistical Evaluation of Models",
    "text": "Statistical Evaluation of Models\n\nBeyond Accuracy: Comprehensive Metrics\nFor classification, we need more than just accuracy:\n\nPrecision: \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\\) - When we predict positive, how often are we correct?\nRecall: \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\) - How many actual positives do we catch?\nF1-score: Harmonic mean of precision and recall\nROC curves: Trade-off between true positive and false positive rates\nAUC: Area under ROC curve, overall performance measure\n\n\n\nCross-Validation and Generalization\nK-fold cross-validation provides better estimates of generalization error:\n\nSplit data into K folds\nFor each fold: train on K-1 folds, test on held-out fold\nAverage performance across all folds\n\nThis helps detect overfitting and provides more reliable performance estimates.\n\n\nStatistical Significance Testing\nWhen comparing models, we need to determine if differences are statistically significant:\n\nPaired t-test: Compare performance across multiple datasets/folds\nMcNemar’s test: Compare classifiers on the same test set\nWilcoxon signed-rank test: Non-parametric alternative to t-test"
  },
  {
    "objectID": "Session4/index.html#common-student-challenges-and-solutions",
    "href": "Session4/index.html#common-student-challenges-and-solutions",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Common Student Challenges and Solutions",
    "text": "Common Student Challenges and Solutions\n\nChallenge 1: “Why do we need probability? Can’t we just use the most likely answer?”\nSolution: Use the “self-driving car” analogy. A car that’s 51% sure the road is clear might proceed, while one that’s only 51% sure might wait for more evidence. Probability gives us the confidence level needed for different decisions.\n\n\nChallenge 2: “Bayes’ theorem seems too abstract and mathematical.”\nSolution: Start with concrete examples like medical testing or spam filtering. Show how prior knowledge (disease prevalence, typical spam patterns) combines with new evidence (test results, email content) to update beliefs.\n\n\nChallenge 3: “How do we choose priors in Bayesian methods?”\nSolution: Explain that priors can be:\n\nInformative: Based on domain knowledge\nWeakly informative: Gently regularize without strong assumptions\n\nNon-informative: Maximally conservative (like uniform priors)\nEmpirical Bayes: Learned from data"
  },
  {
    "objectID": "Session4/index.html#hands-on-exploration-quantifying-uncertainty",
    "href": "Session4/index.html#hands-on-exploration-quantifying-uncertainty",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Hands-On Exploration: Quantifying Uncertainty",
    "text": "Hands-On Exploration: Quantifying Uncertainty\n\nBayesian Linear Regression\nConfidance intervals prediction\n\n\nBayesian Linear Regression\nImplement Bayesian linear regression on Iris data:\n\\[\n\\begin{aligned}\np(\\mathbf{w}|\\alpha) &= \\mathcal{N}(\\mathbf{0}, \\alpha^{-1}\\mathbf{I}) \\\\\\\\\np(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, \\beta) &= \\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\beta^{-1}\\mathbf{I})\n\\end{aligned}\n\\]\nThe posterior distribution over weights is Gaussian, and we can compute predictive distributions that include uncertainty estimates.\n\n\nConfidence Intervals for Predictions\nFor any model, we can bootstrap to get confidence intervals:\n\nResample data with replacement many times\nTrain model on each resample\nMake predictions and compute percentiles\n\nThis provides empirical confidence intervals without distributional assumptions.\n\n\nCalibration Plots\nPlot actual frequency vs predicted probability to assess calibration:\n\nPerfect calibration: Points lie on diagonal\nOverconfident: Points below diagonal (predict too extreme probabilities)\nUnderconfident: Points above diagonal (predict too moderate probabilities)"
  },
  {
    "objectID": "Session4/index.html#mission-mastering-uncertainty",
    "href": "Session4/index.html#mission-mastering-uncertainty",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "Mission: Mastering Uncertainty",
    "text": "Mission: Mastering Uncertainty\n\nAssignment 1: The Bayesian Investigator\nImplement a Bayesian classifier for the Iris dataset. Compare:\n\nMaximum Likelihood Estimation (MLE)\nMaximum A Posteriori (MAP) with different priors\nFull Bayesian inference with posterior predictive distribution\n\nAnalyze how priors affect results and uncertainty estimates.\n\n\nAssignment 2: The Uncertainty Quantifier\nTake a standard neural network classifier and add uncertainty quantification using:\n\nMonte Carlo dropout\nEnsemble methods\nTemperature scaling for calibration\n\nCompare the uncertainty estimates and their reliability.\n\n\nAssignment 3: The Statistical Evaluator\nCompare two classification algorithms using proper statistical testing:\n\nCompute multiple performance metrics\nUse cross-validation for reliable estimates\nPerform statistical significance tests\nCreate confidence intervals for performance differences"
  },
  {
    "objectID": "Session4/index.html#references",
    "href": "Session4/index.html#references",
    "title": "Session 4: Probability & Statistics - The Science of Uncertainty",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Session6/slides-ML.html#the-symphony-of-mathematical-foundations",
    "href": "Session6/slides-ML.html#the-symphony-of-mathematical-foundations",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "The Symphony of Mathematical Foundations",
    "text": "The Symphony of Mathematical Foundations\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#transformers-a-case-study-in-mathematical-integration",
    "href": "Session6/slides-ML.html#transformers-a-case-study-in-mathematical-integration",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Transformers: A Case Study in Mathematical Integration",
    "text": "Transformers: A Case Study in Mathematical Integration\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#convolutional-neural-networks-spatial-intelligence",
    "href": "Session6/slides-ML.html#convolutional-neural-networks-spatial-intelligence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Convolutional Neural Networks: Spatial Intelligence",
    "text": "Convolutional Neural Networks: Spatial Intelligence\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#generative-models-probability-in-action",
    "href": "Session6/slides-ML.html#generative-models-probability-in-action",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Generative Models: Probability in Action",
    "text": "Generative Models: Probability in Action\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#graph-neural-networks-relational-intelligence",
    "href": "Session6/slides-ML.html#graph-neural-networks-relational-intelligence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Graph Neural Networks: Relational Intelligence",
    "text": "Graph Neural Networks: Relational Intelligence\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#reinforcement-learning-sequential-decision-making",
    "href": "Session6/slides-ML.html#reinforcement-learning-sequential-decision-making",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Reinforcement Learning: Sequential Decision Making",
    "text": "Reinforcement Learning: Sequential Decision Making\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#mathematical-patterns-across-architectures",
    "href": "Session6/slides-ML.html#mathematical-patterns-across-architectures",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mathematical Patterns Across Architectures",
    "text": "Mathematical Patterns Across Architectures\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#teaching-integrated-thinking",
    "href": "Session6/slides-ML.html#teaching-integrated-thinking",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Teaching Integrated Thinking",
    "text": "Teaching Integrated Thinking\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#advanced-mathematical-concepts-in-modern-ai",
    "href": "Session6/slides-ML.html#advanced-mathematical-concepts-in-modern-ai",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Advanced Mathematical Concepts in Modern AI",
    "text": "Advanced Mathematical Concepts in Modern AI\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#ethical-ai-mathematical-foundations-of-fairness",
    "href": "Session6/slides-ML.html#ethical-ai-mathematical-foundations-of-fairness",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Ethical AI: Mathematical Foundations of Fairness",
    "text": "Ethical AI: Mathematical Foundations of Fairness\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#mathematical-pattern-recognition",
    "href": "Session6/slides-ML.html#mathematical-pattern-recognition",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mathematical Pattern Recognition",
    "text": "Mathematical Pattern Recognition\nIdentify these patterns in different architectures:\n\nMatrix Factorization Pattern: Autoencoders, PCA, embeddings\nMessage Passing Pattern: GNNs, transformers, CNNs\nComposition Pattern: All deep networks\nOptimization Pattern: All learning algorithms"
  },
  {
    "objectID": "Session6/slides-ML.html#common-integration-challenges-and-solutions",
    "href": "Session6/slides-ML.html#common-integration-challenges-and-solutions",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Common Integration Challenges and Solutions",
    "text": "Common Integration Challenges and Solutions\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#mission-architectural-thinking",
    "href": "Session6/slides-ML.html#mission-architectural-thinking",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mission: Architectural Thinking",
    "text": "Mission: Architectural Thinking\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/slides-ML.html#looking-ahead-teaching-excellence",
    "href": "Session6/slides-ML.html#looking-ahead-teaching-excellence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Looking Ahead: Teaching Excellence",
    "text": "Looking Ahead: Teaching Excellence\n\n\n\n\n\n\nKey points"
  },
  {
    "objectID": "Session6/index.html#the-symphony-of-mathematical-foundations",
    "href": "Session6/index.html#the-symphony-of-mathematical-foundations",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "The Symphony of Mathematical Foundations",
    "text": "The Symphony of Mathematical Foundations\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\nWelcome to our synthesis session, where we witness the beautiful integration of all four mathematical pillars working in concert within modern AI architectures. We have explored each pillar individually—linear algebra as our language, calculus as our learning engine, probability as our framework for uncertainty, and optimization as our strategic guide. Now, we observe how these disciplines combine to create the remarkable capabilities of contemporary AI systems.\nIn this session, we will deconstruct advanced architectures to reveal their mathematical foundations, understand how different pillars dominate different components, and develop the integrated thinking needed to design, analyze, and explain complex AI systems. This is where abstract mathematics transforms into practical intelligence."
  },
  {
    "objectID": "Session6/index.html#transformers-a-case-study-in-mathematical-integration",
    "href": "Session6/index.html#transformers-a-case-study-in-mathematical-integration",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Transformers: A Case Study in Mathematical Integration",
    "text": "Transformers: A Case Study in Mathematical Integration\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe Transformer Architecture Overview\nTransformers have revolutionized natural language processing and beyond. Let’s examine how each mathematical pillar contributes:\nInput: \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) (sequence of n tokens, each d-dimensional)\n\n\nLinear Algebra: The Structural Foundation\nToken Embeddings: \\[\n\\mathbf{E} = \\mathbf{X}\\mathbf{W}_e + \\mathbf{b}_e\n\\] where \\(\\mathbf{W}_e \\in \\mathbb{R}^{d \\times d_{model}}\\) creates dense representations\nPositional Encoding: \\[\nPE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n\\] \\[\nPE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n\\]\n\n\nThe Attention Mechanism: Linear Algebra Meets Probability\nQuery, Key, Value Projections: \\[\n\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\n\\]\nAttention Scores (Linear Algebra): \\[\n\\mathbf{S} = \\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\n\\]\nAttention Weights (Probability): \\[\n\\mathbf{A} = \\text{softmax}(\\mathbf{S}) = \\frac{\\exp(\\mathbf{S}_{ij})}{\\sum_k \\exp(\\mathbf{S}_{ik})}\n\\]\nOutput (Integration): \\[\n\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\mathbf{A}\\mathbf{V}\n\\]\n\n\nCalculus: The Learning Mechanism\nBackpropagation through Attention: The gradients flow through the softmax and matrix multiplications:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_Q} = \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{Q}} \\frac{\\partial \\mathbf{Q}}{\\partial \\mathbf{W}_Q}\n\\]\nThe chain rule enables learning across all parameters simultaneously.\n\n\nOptimization: The Training Strategy\nAdam Optimization with Warmup: Transformers typically use Adam with learning rate scheduling:\n\\[\n\\eta_t = \\eta_{\\text{min}} + \\frac{1}{2}(\\eta_{\\text{max}} - \\eta_{\\text{min}})\\left(1 + \\cos\\left(\\frac{t}{T}\\pi\\right)\\right)\n\\]\n\n\nTransformer Dissection- Hands-On Architecture Analysis\nLet’s analyze a small transformer step-by-step:\n# Input: \"The cat sat\"\ntokens = [\"The\", \"cat\", \"sat\"]\nembeddings = embedding_matrix[tokens]  # Linear Algebra\npositional_encoding = sin_cos_encoding(positions)  # Trigonometry\ninput_vectors = embeddings + positional_encoding  # Linear Algebra\n\n# Self-attention\nQ = input_vectors @ W_Q  # Linear Algebra\nK = input_vectors @ W_K  # Linear Algebra\nV = input_vectors @ W_V  # Linear Algebra\n\nattention_scores = Q @ K.T / sqrt(d_k)  # Linear Algebra\nattention_weights = softmax(attention_scores)  # Probability\ncontext_vectors = attention_weights @ V  # Linear Algebra\n\n# Training\nloss = cross_entropy(predictions, labels)  # Probability + Calculus\ngradients = backpropagate(loss)  # Calculus\noptimizer.step(gradients)  # Optimization"
  },
  {
    "objectID": "Session6/index.html#convolutional-neural-networks-spatial-intelligence",
    "href": "Session6/index.html#convolutional-neural-networks-spatial-intelligence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Convolutional Neural Networks: Spatial Intelligence",
    "text": "Convolutional Neural Networks: Spatial Intelligence\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nMathematical Foundations of Convolution\nDiscrete Convolution (Linear Algebra): \\[\n(\\mathbf{I} * \\mathbf{K})_{ij} = \\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1} \\mathbf{I}_{i+m,j+n} \\mathbf{K}_{m,n}\n\\]\nThis can be represented as a matrix multiplication with a sparse, structured weight matrix.\n\n\nCalculus in CNNs\nBackpropagation through Convolution: The gradient with respect to kernel weights:\n\\[\n\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{K}_{m,n}} = \\sum_{i,j} \\frac{\\partial \\mathcal{L}}{\\partial (\\mathbf{I} * \\mathbf{K})_{ij}} \\mathbf{I}_{i+m,j+n}\n\\]\n\n\nOptimization Considerations\nWeight Initialization: He initialization for ReLU networks: \\[\n\\mathbf{W} \\sim \\mathcal{N}\\left(0, \\frac{2}{n_l}\\right)\n\\]"
  },
  {
    "objectID": "Session6/index.html#generative-models-probability-in-action",
    "href": "Session6/index.html#generative-models-probability-in-action",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Generative Models: Probability in Action",
    "text": "Generative Models: Probability in Action\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nVariational Autoencoders (VAEs)\nProbabilistic Framework: \\[\np_\\theta(\\mathbf{x}) = \\int p_\\theta(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})d\\mathbf{z}\n\\]\nVariational Inference (Probability + Optimization): We maximize the Evidence Lower Bound (ELBO):\n\\[\n\\mathcal{L}(\\theta, \\phi; \\mathbf{x}) = \\mathbb{E}_{q_\\phi(\\mathbf{z}|\\mathbf{x})}[\\log p_\\theta(\\mathbf{x}|\\mathbf{z})] - D_{KL}(q_\\phi(\\mathbf{z}|\\mathbf{x}) \\| p(\\mathbf{z}))\n\\]\nReparameterization Trick (Calculus): \\[\n\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\odot \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})\n\\]\nThis enables gradient flow through stochastic nodes.\n\n\nGenerative Adversarial Networks (GANs)\nGame Theoretic Optimization: \\[\n\\min_G \\max_D V(D, G) = \\mathbb{E}_{\\mathbf{x} \\sim p_{data}}[\\log D(\\mathbf{x})] + \\mathbb{E}_{\\mathbf{z} \\sim p(\\mathbf{z})}[\\log(1 - D(G(\\mathbf{z})))]\n\\]\nTraining Dynamics (Calculus + Optimization): Alternating gradient updates:\n\\[\n\\theta_D^{(t+1)} = \\theta_D^{(t)} + \\eta \\nabla_{\\theta_D} V(D, G)\n\\] \\[\n\\theta_G^{(t+1)} = \\theta_G^{(t)} - \\eta \\nabla_{\\theta_G} V(D, G)\n\\]"
  },
  {
    "objectID": "Session6/index.html#graph-neural-networks-relational-intelligence",
    "href": "Session6/index.html#graph-neural-networks-relational-intelligence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Graph Neural Networks: Relational Intelligence",
    "text": "Graph Neural Networks: Relational Intelligence\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nMessage Passing Framework\nLinear Algebra for Graph Structure: Adjacency matrix \\(\\mathbf{A}\\) and node features \\(\\mathbf{X}\\)\nMessage Passing (Linear Algebra + Calculus): \\[\n\\mathbf{H}^{(l+1)} = \\sigma\\left(\\mathbf{A}\\mathbf{H}^{(l)}\\mathbf{W}^{(l)}\\right)\n\\]\nGraph Attention (Integration): \\[\n\\alpha_{ij} = \\frac{\\exp(\\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{W}\\mathbf{h}_i \\| \\mathbf{W}\\mathbf{h}_j]))}{\\sum_{k \\in \\mathcal{N}(i)} \\exp(\\text{LeakyReLU}(\\mathbf{a}^T[\\mathbf{W}\\mathbf{h}_i \\| \\mathbf{W}\\mathbf{h}_k]))}\n\\]"
  },
  {
    "objectID": "Session6/index.html#reinforcement-learning-sequential-decision-making",
    "href": "Session6/index.html#reinforcement-learning-sequential-decision-making",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Reinforcement Learning: Sequential Decision Making",
    "text": "Reinforcement Learning: Sequential Decision Making\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nMarkov Decision Processes\nProbabilistic State Transitions: \\[\nP(s_{t+1} | s_t, a_t)\n\\]\nValue Functions (Probability + Optimization): Bellman equation:\n\\[\nV^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^\\infty \\gamma^t r_t | s_0 = s\\right]\n\\]\n\n\nPolicy Gradient Methods\nScore Function Gradient (Calculus): \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)\\right]\n\\]"
  },
  {
    "objectID": "Session6/index.html#mathematical-patterns-across-architectures",
    "href": "Session6/index.html#mathematical-patterns-across-architectures",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mathematical Patterns Across Architectures",
    "text": "Mathematical Patterns Across Architectures\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nCommon Mathematical Motifs\nComposition of Functions: All deep learning architectures are compositions: \\[\nf(\\mathbf{x}) = f_L(f_{L-1}(\\dots f_1(\\mathbf{x})\\dots))\n\\]\nOptimization Landscapes: Different architectures create different loss landscapes:\n\nCNNs: Relatively smooth due to weight sharing\nTransformers: High-dimensional, many saddle points\nGANs: Non-convex, non-concave minimax games\n\nInformation Flow: Mathematics describes how information transforms through networks:\n\nLinear transformations change representations\nNon-linearities enable complex functions\nSkip connections preserve information"
  },
  {
    "objectID": "Session6/index.html#teaching-integrated-thinking",
    "href": "Session6/index.html#teaching-integrated-thinking",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Teaching Integrated Thinking",
    "text": "Teaching Integrated Thinking\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nThe “Orchestra” Analogy\nEach mathematical pillar is like a section in an orchestra:\n\nLinear Algebra: String section - provides foundation and structure\nCalculus: Percussion section - drives rhythm and progression\nProbability: Woodwinds - adds color and nuance\nOptimization: Conductor - coordinates and directs\n\nBeautiful music emerges only when all sections work together harmoniously.\n\n\nThe “Building Construction” Metaphor\n\nLinear Algebra: Blueprints and materials\nCalculus: Construction techniques and processes\nProbability: Safety factors and uncertainty margins\nOptimization: Project management and resource allocation\n\n\n\nCase Study Approach\nAnalyze real architectures by asking:\n\nLinear Algebra: How is data represented and transformed?\nCalculus: How does learning happen?\nProbability: How is uncertainty handled?\nOptimization: What strategy finds good solutions?"
  },
  {
    "objectID": "Session6/index.html#advanced-mathematical-concepts-in-modern-ai",
    "href": "Session6/index.html#advanced-mathematical-concepts-in-modern-ai",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Advanced Mathematical Concepts in Modern AI",
    "text": "Advanced Mathematical Concepts in Modern AI\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nManifold Learning\n\nMathematical Insight: High-dimensional data often lies on low-dimensional manifolds\nLinear Algebra Perspective: Tangent spaces and embeddings\nProbability Perspective: Density estimation on manifolds\nOptimization Perspective: Preserving local and global structure\n\n\n\nInformation Theory in Deep Learning\nMutual Information: \\[\nI(X; Y) = \\mathbb{E}_{p(x,y)}\\left[\\log\\frac{p(x,y)}{p(x)p(y)}\\right]\n\\]\nUsed in:\n\nInformation bottlenecks\nDisentangled representations\nContrastive learning\n\n\n\nGeometric Deep Learning\nExtending neural networks to non-Euclidean domains using:\n\nGroup theory (symmetries)\nDifferential geometry (manifolds)\nAlgebraic topology (connectivity)"
  },
  {
    "objectID": "Session6/index.html#ethical-ai-mathematical-foundations-of-fairness",
    "href": "Session6/index.html#ethical-ai-mathematical-foundations-of-fairness",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Ethical AI: Mathematical Foundations of Fairness",
    "text": "Ethical AI: Mathematical Foundations of Fairness\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nMathematical Definitions of Fairness\nDemographic Parity: \\[\nP(\\hat{Y} = 1 | A = a) = P(\\hat{Y} = 1 | A = b) \\quad \\forall a,b\n\\]\nEqualized Odds: \\[\nP(\\hat{Y} = 1 | A = a, Y = y) = P(\\hat{Y} = 1 | A = b, Y = y) \\quad \\forall a,b,y\n\\]\n\n\nOptimization with Fairness Constraints\nConstrained Optimization: \\[\n\\min_\\theta \\mathcal{L}(\\theta) \\quad \\text{subject to} \\quad \\text{Fairness}(\\theta) \\leq \\epsilon\n\\]\nLagrangian Approach: \\[\n\\mathcal{L}(\\theta, \\lambda) = \\mathcal{L}(\\theta) + \\lambda \\cdot \\text{Fairness}(\\theta)\n\\]"
  },
  {
    "objectID": "Session6/index.html#mathematical-pattern-recognition",
    "href": "Session6/index.html#mathematical-pattern-recognition",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mathematical Pattern Recognition",
    "text": "Mathematical Pattern Recognition\nIdentify these patterns in different architectures:\n\nMatrix Factorization Pattern: Autoencoders, PCA, embeddings\nMessage Passing Pattern: GNNs, transformers, CNNs\nComposition Pattern: All deep networks\nOptimization Pattern: All learning algorithms"
  },
  {
    "objectID": "Session6/index.html#common-integration-challenges-and-solutions",
    "href": "Session6/index.html#common-integration-challenges-and-solutions",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Common Integration Challenges and Solutions",
    "text": "Common Integration Challenges and Solutions\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nChallenge 1: “The math seems disconnected from implementation”\nSolution: Use mathematical tracing:\n\nStart with forward pass equations\nMap each equation to code\nTrack dimensions through transformations\nVisualize intermediate representations\n\n\n\nChallenge 2: “How do I know which mathematical concept to apply?”\nSolution: Develop diagnostic questions:\n\nIs this about representation? → Linear Algebra\nIs this about learning? → Calculus\nIs this about uncertainty? → Probability\nIs this about search? → Optimization\n\n\n\nChallenge 3: “The architectures keep changing”\nSolution: Focus on mathematical principles:\n\nNew architectures are combinations of known mathematical patterns\nUnderstand the fundamental operations\nLearn to recognize mathematical motifs"
  },
  {
    "objectID": "Session6/index.html#mission-architectural-thinking",
    "href": "Session6/index.html#mission-architectural-thinking",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Mission: Architectural Thinking",
    "text": "Mission: Architectural Thinking\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\n\nAssignment 1: The Mathematical Archeologist\nChoose one modern architecture (Vision Transformer, Diffusion Models, etc.) and:\n\nIdentify all four mathematical pillars in its design\nTrace how information flows mathematically\nAnalyze the optimization strategy\nCreate a “mathematical map” of the architecture\n\n\n\nAssignment 2: The Integrated Explainer\nTake a complex AI concept and explain it using integrated mathematical thinking:\n\nUse analogies from multiple pillars\nShow how different mathematical disciplines interact\nCreate visual explanations connecting the mathematics\nDemonstrate why all pillars are necessary\n\n\n\nAssignment 3: The Architecture Designer\nDesign a simple novel architecture for a specific problem:\n\nSpecify the mathematical components from each pillar\nJustify your design choices mathematically\nPredict the optimization challenges\nPropose evaluation methods"
  },
  {
    "objectID": "Session6/index.html#looking-ahead-teaching-excellence",
    "href": "Session6/index.html#looking-ahead-teaching-excellence",
    "title": "Session 6: Pillars in Practice - Advanced Architectures and Integrated Thinking",
    "section": "Looking Ahead: Teaching Excellence",
    "text": "Looking Ahead: Teaching Excellence\n\n\n\n\n\n\nKey points\n\n\n\n\n\n\nIn our final session, we will focus on pedagogical excellence—how to effectively teach these integrated mathematical concepts, create engaging learning experiences, and build a community of mathematically-grounded AI educators. We’ll practice micro-teaching, develop assessment strategies, and create sustainable teaching practices.\nPrepare by considering: How will you explain these integrated concepts to your students? What analogies and examples work best? How can you make abstract mathematics feel concrete and relevant? How will you assess true understanding rather than memorization?\n\n\n\n\n\n\nNote\n\n\n\nRemember: The ultimate goal is not just to understand these mathematical foundations ourselves, but to become educators who can illuminate these concepts for others. The future of AI education depends on teachers who can reveal the mathematical beauty underlying the technological marvels."
  }
]